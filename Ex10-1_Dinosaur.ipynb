{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 10-1. Character level language model - Dinosaur\n",
    "\n",
    "공룡 이름 생성기 만들기 \n",
    "\n",
    "- 기존의 공룡 이름 데이터를 사용하여 RNN 모형을 만들고 자동으로 이름 생성 \n",
    "- Sequence-to-sequence 형태의 모형을 사용 \n",
    "\n",
    "<img src=\"figures/dinosour.png\" width=\"70%\">\n",
    "<img src=\"figures/dinosour2.png\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T09:23:02.107800Z",
     "start_time": "2019-03-02T09:23:00.690210Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 데이터 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이름 사이는 enter(`\\n`)가 들어가 있음\n",
    "- 이름의 시작을 tab(`\\t`)으로 구분할 것이기 때문에 `\\t`를 `\\chars`에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T09:47:35.620161Z",
     "start_time": "2019-03-02T09:47:35.608773Z"
    }
   },
   "outputs": [],
   "source": [
    "data = open('data/dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "chars.append('\\t') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "TO DO:\n",
    "    \n",
    "- 데이터가 전체는 총 몇 개의 문자로 이루어져 있는가?\n",
    "- Vocabulary는 몇 개의 문자로 이루어져 있는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19909 28\n"
     ]
    }
   ],
   "source": [
    "# Your answer comes here\n",
    "print(len(data),\n",
    "len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    \n",
    "TO DO:\n",
    "\n",
    "- char와 index를 연결하는 lookup table을 만드시오. (`char2index`, `index2char`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer comes here\n",
    "char2index = dict((c, i) for i, c in enumerate(sorted(chars)))\n",
    "index2char = dict((i, c) for i, c in enumerate(sorted(chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data 만들기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    \n",
    "TO DO: \n",
    "    \n",
    "- `examples`의 이름으로 각 줄을 읽어들임\n",
    "- 공백을 없애고 소문자로 변환하여 이름 list 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "INPUT_FILE = \"data/dinos.txt\"\n",
    "\n",
    "fin = open(INPUT_FILE, 'rb') # 바이너리 파일을 읽기 모드로 오픈\n",
    "\n",
    "examples = []\n",
    "i=0\n",
    "for line in fin: # 파일을 한 줄씩 읽어들임\n",
    "    line = line.strip().lower() # 공백을 제거하고 소문자로 변환\n",
    "    line = line.decode(\"ascii\") # 디코딩하여 char로 변환\n",
    "    if len(line) == 0: # 빈 줄 삭제\n",
    "        continue\n",
    "    examples.append(line)\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    \n",
    "TO DO: \n",
    "    \n",
    "- 이름의 최대 길이는 몇 인가? 즉, 가장 긴 이름은 몇 개의 문자로 이루어져 있는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer comes here\n",
    "max_len = max([len(line) for line in examples])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성할 가장 긴 이름을 `\\n`, `\\t` 포함 30개 문자로 이루어지도록 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T09:48:10.993741Z",
     "start_time": "2019-03-02T09:48:10.990820Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "nb_chars = len(char2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    \n",
    "TO DO: \n",
    "    \n",
    "아래의 사항을 포함하여 문자로 이루어진 이름을 각 문자를 나타내는 one-hot vector의 시퀀스로 변환하여 sequence-to-sequence 모델의 input(X)와 output(Y)를 만드시오. \n",
    "\n",
    "- 이름의 시작에는 `\\t`붙이기\n",
    "- 이름의 끝에는 `\\n` 붙이기\n",
    "- 총 이름의 길이는 `max_len` (`max_len`보다 짧은 경우 뒤를 `\\n`으로 채움) \n",
    "- Y는 X 입력값의 다음 문자를 지정 \n",
    "\n",
    "- E.g. example[0]: aachenosaurus\n",
    "      input: \\t a a c h e n n o s a u r u s \\n \\n \\n .... (30개 채울 때 까지)\n",
    "      output: a a c h e n n o s a u r u s \\n \\n \\n .... (30개 채울 때 까지)\n",
    "        \n",
    "        X[0]= array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0],\n",
    "        ...\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(examples), max_len, nb_chars), dtype=int)\n",
    "Y = np.zeros((len(examples), max_len, nb_chars), dtype=int)\n",
    "\n",
    "for i, example in enumerate(examples):\n",
    "    example_x = '\\t'+example\n",
    "    if max_len > len(example_x):\n",
    "        example_x = example_x +'\\n' * (max_len - len(example_x))\n",
    "        example_y = example + '\\n' * (max_len - len(example))\n",
    "    for j, character in enumerate(example_x):\n",
    "        X[i, j, char2index[character]] = 1.\n",
    "    for j, character in enumerate(example_y):\n",
    "        Y[i, j, char2index[character]] = 1.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T09:49:38.507088Z",
     "start_time": "2019-03-02T09:49:38.502373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1536, 30, 28), (1536, 30, 28))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T09:49:50.413360Z",
     "start_time": "2019-03-02T09:49:50.408140Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T09:49:51.099686Z",
     "start_time": "2019-03-02T09:49:50.864589Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 64)            23808     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 64)            33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30, 64)            4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30, 28)            1820      \n",
      "=================================================================\n",
      "Total params: 62,812\n",
      "Trainable params: 62,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Your answer comes here\n",
    "HIDDEN_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=True, input_shape=(max_len, nb_chars), activation='relu'))\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=True, activation='relu'))\n",
    "model.add(Dense(HIDDEN_SIZE, activation='relu'))\n",
    "model.add(Dense(nb_chars, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.adam(lr=0.001))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1228 samples, validate on 308 samples\n",
      "Epoch 1/100\n",
      "1228/1228 [==============================] - 2s 2ms/step - loss: 3.2720 - val_loss: 3.1611\n",
      "Epoch 2/100\n",
      "1228/1228 [==============================] - 1s 681us/step - loss: 2.7813 - val_loss: 1.6142\n",
      "Epoch 3/100\n",
      "1228/1228 [==============================] - 1s 669us/step - loss: 1.5848 - val_loss: 1.4614\n",
      "Epoch 4/100\n",
      "1228/1228 [==============================] - 1s 670us/step - loss: 1.4327 - val_loss: 1.4008\n",
      "Epoch 5/100\n",
      "1228/1228 [==============================] - 1s 669us/step - loss: 1.3873 - val_loss: 1.3061\n",
      "Epoch 6/100\n",
      "1228/1228 [==============================] - 1s 686us/step - loss: 1.3314 - val_loss: 1.2769\n",
      "Epoch 7/100\n",
      "1228/1228 [==============================] - 1s 666us/step - loss: 1.3022 - val_loss: 1.2664\n",
      "Epoch 8/100\n",
      "1228/1228 [==============================] - 1s 685us/step - loss: 1.2870 - val_loss: 1.2466\n",
      "Epoch 9/100\n",
      "1228/1228 [==============================] - 1s 668us/step - loss: 1.2748 - val_loss: 1.2403\n",
      "Epoch 10/100\n",
      "1228/1228 [==============================] - 1s 671us/step - loss: 1.2638 - val_loss: 1.2273\n",
      "Epoch 11/100\n",
      "1228/1228 [==============================] - 1s 675us/step - loss: 1.2573 - val_loss: 1.2222\n",
      "Epoch 12/100\n",
      "1228/1228 [==============================] - 1s 672us/step - loss: 1.2506 - val_loss: 1.2163\n",
      "Epoch 13/100\n",
      "1228/1228 [==============================] - 1s 677us/step - loss: 1.2402 - val_loss: 1.2071\n",
      "Epoch 14/100\n",
      "1228/1228 [==============================] - 1s 660us/step - loss: 1.2306 - val_loss: 1.1996\n",
      "Epoch 15/100\n",
      "1228/1228 [==============================] - 1s 682us/step - loss: 1.2226 - val_loss: 1.1930\n",
      "Epoch 16/100\n",
      "1228/1228 [==============================] - 1s 669us/step - loss: 1.2147 - val_loss: 1.1876\n",
      "Epoch 17/100\n",
      "1228/1228 [==============================] - 1s 686us/step - loss: 1.2083 - val_loss: 1.1805\n",
      "Epoch 18/100\n",
      "1228/1228 [==============================] - 1s 676us/step - loss: 1.2019 - val_loss: 1.1757\n",
      "Epoch 19/100\n",
      "1228/1228 [==============================] - 1s 663us/step - loss: 1.1959 - val_loss: 1.1720\n",
      "Epoch 20/100\n",
      "1228/1228 [==============================] - 1s 664us/step - loss: 1.1901 - val_loss: 1.1660\n",
      "Epoch 21/100\n",
      "1228/1228 [==============================] - 1s 679us/step - loss: 1.1852 - val_loss: 1.1616\n",
      "Epoch 22/100\n",
      "1228/1228 [==============================] - 1s 666us/step - loss: 1.1790 - val_loss: 1.1574\n",
      "Epoch 23/100\n",
      "1228/1228 [==============================] - 1s 660us/step - loss: 1.1720 - val_loss: 1.1497\n",
      "Epoch 24/100\n",
      "1228/1228 [==============================] - 1s 678us/step - loss: 1.1615 - val_loss: 1.1383\n",
      "Epoch 25/100\n",
      "1228/1228 [==============================] - 1s 667us/step - loss: 1.1465 - val_loss: 1.1241\n",
      "Epoch 26/100\n",
      "1228/1228 [==============================] - 1s 672us/step - loss: 1.1287 - val_loss: 1.1055\n",
      "Epoch 27/100\n",
      "1228/1228 [==============================] - 1s 684us/step - loss: 1.1092 - val_loss: 1.0928\n",
      "Epoch 28/100\n",
      "1228/1228 [==============================] - 1s 663us/step - loss: 1.0896 - val_loss: 1.0727\n",
      "Epoch 29/100\n",
      "1228/1228 [==============================] - 1s 650us/step - loss: 1.0697 - val_loss: 1.0603\n",
      "Epoch 30/100\n",
      "1228/1228 [==============================] - 1s 655us/step - loss: 1.0523 - val_loss: 1.0506\n",
      "Epoch 31/100\n",
      "1228/1228 [==============================] - 1s 676us/step - loss: 1.0378 - val_loss: 1.0343\n",
      "Epoch 32/100\n",
      "1228/1228 [==============================] - 1s 661us/step - loss: 1.0208 - val_loss: 1.0213\n",
      "Epoch 33/100\n",
      "1228/1228 [==============================] - 1s 662us/step - loss: 1.0088 - val_loss: 1.0075\n",
      "Epoch 34/100\n",
      "1228/1228 [==============================] - 1s 676us/step - loss: 1.0568 - val_loss: 1.0293\n",
      "Epoch 35/100\n",
      "1228/1228 [==============================] - 1s 648us/step - loss: 1.0125 - val_loss: 1.0172\n",
      "Epoch 36/100\n",
      "1228/1228 [==============================] - 1s 646us/step - loss: 0.9994 - val_loss: 1.0025\n",
      "Epoch 37/100\n",
      "1228/1228 [==============================] - 1s 666us/step - loss: 0.9825 - val_loss: 0.9917\n",
      "Epoch 38/100\n",
      "1228/1228 [==============================] - 1s 682us/step - loss: 0.9806 - val_loss: 0.9829\n",
      "Epoch 39/100\n",
      "1228/1228 [==============================] - 1s 660us/step - loss: 0.9670 - val_loss: 0.9767\n",
      "Epoch 40/100\n",
      "1228/1228 [==============================] - 1s 669us/step - loss: 0.9532 - val_loss: 0.9698\n",
      "Epoch 41/100\n",
      "1228/1228 [==============================] - 1s 668us/step - loss: 0.9462 - val_loss: 0.9657\n",
      "Epoch 42/100\n",
      "1228/1228 [==============================] - 1s 664us/step - loss: 0.9394 - val_loss: 0.9601\n",
      "Epoch 43/100\n",
      "1228/1228 [==============================] - 1s 676us/step - loss: 0.9323 - val_loss: 0.9571\n",
      "Epoch 44/100\n",
      "1228/1228 [==============================] - 1s 660us/step - loss: 0.9253 - val_loss: 0.9519\n",
      "Epoch 45/100\n",
      "1228/1228 [==============================] - 1s 665us/step - loss: 0.9228 - val_loss: 0.9548\n",
      "Epoch 46/100\n",
      "1228/1228 [==============================] - 1s 660us/step - loss: 0.9159 - val_loss: 0.9430\n",
      "Epoch 47/100\n",
      "1228/1228 [==============================] - 1s 672us/step - loss: 0.9076 - val_loss: 0.9386\n",
      "Epoch 48/100\n",
      "1228/1228 [==============================] - 1s 668us/step - loss: 0.8949 - val_loss: 0.9322\n",
      "Epoch 49/100\n",
      "1228/1228 [==============================] - 1s 698us/step - loss: 0.8835 - val_loss: 0.9250\n",
      "Epoch 50/100\n",
      "1228/1228 [==============================] - 1s 672us/step - loss: 0.8741 - val_loss: 0.9207\n",
      "Epoch 51/100\n",
      "1228/1228 [==============================] - 1s 674us/step - loss: 0.8651 - val_loss: 0.9170\n",
      "Epoch 52/100\n",
      "1228/1228 [==============================] - 1s 668us/step - loss: 0.8590 - val_loss: 0.9139\n",
      "Epoch 53/100\n",
      "1228/1228 [==============================] - 1s 664us/step - loss: 0.8507 - val_loss: 0.9079\n",
      "Epoch 54/100\n",
      "1228/1228 [==============================] - 1s 666us/step - loss: 0.8426 - val_loss: 0.9055\n",
      "Epoch 55/100\n",
      "1228/1228 [==============================] - 1s 678us/step - loss: 0.8361 - val_loss: 0.9024\n",
      "Epoch 56/100\n",
      "1228/1228 [==============================] - 1s 668us/step - loss: 0.8305 - val_loss: 0.9036\n",
      "Epoch 57/100\n",
      "1228/1228 [==============================] - 1s 651us/step - loss: 0.8272 - val_loss: 0.9025\n",
      "Epoch 58/100\n",
      "1228/1228 [==============================] - 1s 650us/step - loss: 0.8222 - val_loss: 0.9021\n",
      "Epoch 59/100\n",
      "1228/1228 [==============================] - 1s 669us/step - loss: 0.8157 - val_loss: 0.8966\n",
      "Epoch 60/100\n",
      "1228/1228 [==============================] - 1s 663us/step - loss: 0.8118 - val_loss: 0.8966\n",
      "Epoch 61/100\n",
      "1228/1228 [==============================] - 1s 640us/step - loss: 0.8066 - val_loss: 0.9001\n",
      "Epoch 62/100\n",
      "1228/1228 [==============================] - 1s 630us/step - loss: 0.8039 - val_loss: 1.0284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5330d8d908>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.strftime(\"%c\")\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(filepath='Dinosaur_gen.h5', monitor='val_loss', save_best_only=True),\n",
    "    TensorBoard(log_dir='/home/work/logs/Dinosaur_generation/'+now),\n",
    "    EarlyStopping(monitor='val_loss',patience=3)\n",
    "]\n",
    "model.fit(X, Y, batch_size=BATCH_SIZE, epochs=100, validation_split=0.2, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공룡 이름 생성하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-02T10:09:52.520615Z",
     "start_time": "2019-03-02T10:09:51.006016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tjajhtocronsaurus\n",
      "\tbonlausaurus\n",
      "\teparorarator\n",
      "\twpniianosaurus\n",
      "\tspeicaropsaurus\n",
      "\tppuacletocecpsulus\n",
      "\thhocrlorkax\n",
      "\tdanongonnosaurusysxurasaesaor\n",
      "\tlogmotan\n",
      "\teadospenlus\n",
      "\thnfnatontoplitas\n",
      "\trykiexathus\n",
      "\tcoceratyrax\n",
      "\tthaqbaunn\n",
      "\tdhrortosurus\n",
      "\twrbosurus\n",
      "\taihaocoralosausaurasrus\n",
      "\tjgothansaurus\n",
      "\tidapirodesaurus\n",
      "\tiunatprosaurus\n",
      "\tcorosaurus\n",
      "\tsanonanisaurus\n",
      "\tjolkcosxathosaulus\n",
      "\tdegnaisurus\n",
      "\tgnnsaiepesaurus\n",
      "\toutnposaurus\n",
      "\teproi\n",
      "\tclelaynus\n",
      "\tkomolinasorus\n",
      "\tonansgraphis\n",
      "\tanjyasaurus\n",
      "\tndnirosaurus\n",
      "\teirchhanamus\n",
      "\tgceasinis\n",
      "\taeugderamos\n",
      "\tanemeriosaurus\n",
      "\tcmanopnnysaurus\n",
      "\taiwnosaurus\n",
      "\tbistesaurus\n",
      "\tmudrimiodosaurus\n",
      "\tageharoshurus\n",
      "\tianinamas\n",
      "\tbanctonusaurus\n",
      "\tradrolonatosachus\n",
      "\tmanokanatan\n",
      "\tdeeyerangosaurus\n",
      "\teanlyricoraus\n",
      "\tdprasaurivosaurus\n",
      "\tiraesaurus\n",
      "\tidicanax\n",
      "\tdcerolaceroppx\n",
      "\tduontemtops\n",
      "\tgagisasurus\n",
      "\teuserapenisorurisaurus\n",
      "\taamanamanx\n",
      "\tcblionon\n",
      "\tcprlaporan\n",
      "\tctianhinysaurus\n",
      "\tpegeosaurus\n",
      "\tlancodrahantor\n",
      "\tbacplomosorus\n",
      "\tprotanhigvoras\n",
      "\tlharesitesaurus\n",
      "\tboritopsgus\n",
      "\teelansionoras\n",
      "\tmuuatrosaurus\n",
      "\tdrzantosaurusdls\n",
      "\tsmhyosaurus\n",
      "\tpiroypops\n",
      "\tanaareroftergytons\n",
      "\tiesybolenthosaurus\n",
      "\tklhytonas\n",
      "\tlarrfynsaurus\n",
      "\tguhonovomatons\n",
      "\tsaniton\n",
      "\tpontennhasurus\n",
      "\tbebgesaurus\n",
      "\tkaluesaurus\n",
      "\tcroprotan\n",
      "\tdrsatarurages\n",
      "\treorasurus\n",
      "\tlytylonlhanips\n",
      "\tleeteoratodr\n",
      "\tlrgociontodhus\n",
      "\toarsaknansaurus\n",
      "\tauronhasaurus\n",
      "\tkuaforaurosaurus\n",
      "\tyjauphiophosaurus\n",
      "\thowgunsaurus\n",
      "\tsaplododantops\n",
      "\tdajihecoratosaurus\n",
      "\tggidorlisaurus\n",
      "\tlivytisaurus\n",
      "\tbonnnanteciisur\n",
      "\tpposogox\n",
      "\tryuresaurus\n",
      "\tpiuufceesaurus\n",
      "\tpuagawatorasaurbns\n",
      "\tceeolhaponesanus\n",
      "\tdaltornasaurus\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    stop = False\n",
    "    ch = \"\\t\"\n",
    "    counter = 1\n",
    "    target_seq = np.zeros((1,max_len, nb_chars))\n",
    "    target_seq[0, 0, char2index[ch]] = 1\n",
    "    while stop == False and counter < max_len:\n",
    "        probs = model.predict_proba(target_seq, verbose=0)[:, counter-1, :]\n",
    "        c = np.random.choice(sorted(list(chars)), replace=False, \n",
    "                             p=probs.reshape(nb_chars))\n",
    "        if c==\"\\n\":\n",
    "            stop=True\n",
    "        else:\n",
    "            ch=ch+c\n",
    "            target_seq[0, counter, char2index[c]] = 1\n",
    "            counter = counter + 1\n",
    "    print(ch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
