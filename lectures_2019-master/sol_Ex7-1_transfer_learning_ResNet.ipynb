{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ex 7-1. Transfer Learning with GAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Average Pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50 convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102858752/102853048 [==============================] - 35s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "resnet=ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 56, 56, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 56, 56, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 28, 28, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 14, 14, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,636,712\n",
      "Trainable params: 25,583,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output layer 전 단계에 Global Average Pooling(GAP) layer가 있음\n",
    "\n",
    "![](https://alexisbcook.github.io/assets/global_average_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막 layer에서 flatten을 통해 fully-connected layer 대신 GAP layer를 사용\n",
    "- 하나의 channel(feature map)의 값을 모두 평균을 취하는 layer\n",
    "- fully-connected layer로 연결되기 전에 node 개수를 줄여 학습해야 할 weight의 수를 획기적으로 줄임\n",
    "- fully-connected layer 이전 단계들에서 feature가 충분히 학습되었다고 판단\n",
    "- overfitting의 가능성을 낮추고 획기적으로 빠르게 학습 가능 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16의 convolution base에 global average pooling layer를 사용하여 feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-14T05:18:14.050645Z",
     "start_time": "2018-07-14T05:17:15.075851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-14T05:35:15.988207Z",
     "start_time": "2018-07-14T05:35:15.977283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation, test 이미지가 들어있는 폴더 경로를 지정\n",
    "train_dir = './data/cats_and_dogs_small/train'\n",
    "validation_dir = './data/cats_and_dogs_small/validation'\n",
    "test_dir = './data/cats_and_dogs_small/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(GlobalAveragePooling2D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = model.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 512), (2000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 1s 553us/step - loss: 0.6614 - acc: 0.6135 - val_loss: 0.6333 - val_acc: 0.7210\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.6172 - acc: 0.7370 - val_loss: 0.5964 - val_acc: 0.7820\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.5829 - acc: 0.7780 - val_loss: 0.5643 - val_acc: 0.8140\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.5526 - acc: 0.7905 - val_loss: 0.5373 - val_acc: 0.8290\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.5279 - acc: 0.7970 - val_loss: 0.5150 - val_acc: 0.8320\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.5064 - acc: 0.8055 - val_loss: 0.4944 - val_acc: 0.8360\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.4877 - acc: 0.8110 - val_loss: 0.4772 - val_acc: 0.8370\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.4709 - acc: 0.8190 - val_loss: 0.4608 - val_acc: 0.8410\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.4560 - acc: 0.8280 - val_loss: 0.4478 - val_acc: 0.8420\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.4430 - acc: 0.8350 - val_loss: 0.4370 - val_acc: 0.8420\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.4317 - acc: 0.8305 - val_loss: 0.4244 - val_acc: 0.8450\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.4203 - acc: 0.8365 - val_loss: 0.4128 - val_acc: 0.8540\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.4111 - acc: 0.8465 - val_loss: 0.4043 - val_acc: 0.8520\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.4015 - acc: 0.8460 - val_loss: 0.3976 - val_acc: 0.8460\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.3929 - acc: 0.8530 - val_loss: 0.3895 - val_acc: 0.8520\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.3857 - acc: 0.8465 - val_loss: 0.3803 - val_acc: 0.8560\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.3788 - acc: 0.8535 - val_loss: 0.3749 - val_acc: 0.8600\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.3721 - acc: 0.8570 - val_loss: 0.3676 - val_acc: 0.8640\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.3659 - acc: 0.8595 - val_loss: 0.3631 - val_acc: 0.8620\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.3601 - acc: 0.8670 - val_loss: 0.3559 - val_acc: 0.8670\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.3550 - acc: 0.8660 - val_loss: 0.3516 - val_acc: 0.8720\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.3501 - acc: 0.8670 - val_loss: 0.3491 - val_acc: 0.8650\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.3457 - acc: 0.8650 - val_loss: 0.3426 - val_acc: 0.8730\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.3411 - acc: 0.8720 - val_loss: 0.3388 - val_acc: 0.8730\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.3366 - acc: 0.8700 - val_loss: 0.3365 - val_acc: 0.8720\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.3328 - acc: 0.8740 - val_loss: 0.3320 - val_acc: 0.8730\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 0.3288 - acc: 0.8740 - val_loss: 0.3281 - val_acc: 0.8740\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.3250 - acc: 0.8760 - val_loss: 0.3245 - val_acc: 0.8750\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.3213 - acc: 0.8780 - val_loss: 0.3206 - val_acc: 0.8790\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.3183 - acc: 0.8815 - val_loss: 0.3181 - val_acc: 0.8790\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.3147 - acc: 0.8830 - val_loss: 0.3179 - val_acc: 0.8790\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.3119 - acc: 0.8785 - val_loss: 0.3134 - val_acc: 0.8780\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.3091 - acc: 0.8825 - val_loss: 0.3106 - val_acc: 0.8790\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.3058 - acc: 0.8815 - val_loss: 0.3078 - val_acc: 0.8790\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.3035 - acc: 0.8855 - val_loss: 0.3067 - val_acc: 0.8780\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.3008 - acc: 0.8865 - val_loss: 0.3039 - val_acc: 0.8790\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 0s 116us/step - loss: 0.2979 - acc: 0.8900 - val_loss: 0.3008 - val_acc: 0.8860\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.2960 - acc: 0.8880 - val_loss: 0.2993 - val_acc: 0.8820\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.2936 - acc: 0.8890 - val_loss: 0.2988 - val_acc: 0.8800\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.2912 - acc: 0.8870 - val_loss: 0.2962 - val_acc: 0.8810\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.2892 - acc: 0.8895 - val_loss: 0.2947 - val_acc: 0.8820\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.2871 - acc: 0.8900 - val_loss: 0.2920 - val_acc: 0.8840\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.2851 - acc: 0.8925 - val_loss: 0.2927 - val_acc: 0.8840\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2831 - acc: 0.8915 - val_loss: 0.2888 - val_acc: 0.8860\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.2814 - acc: 0.8920 - val_loss: 0.2892 - val_acc: 0.8850\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.2792 - acc: 0.8930 - val_loss: 0.2859 - val_acc: 0.8870\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.2779 - acc: 0.8900 - val_loss: 0.2864 - val_acc: 0.8870\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.2760 - acc: 0.8935 - val_loss: 0.2840 - val_acc: 0.8860\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.2743 - acc: 0.8945 - val_loss: 0.2849 - val_acc: 0.8860\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.2716 - acc: 0.8945 - val_loss: 0.2809 - val_acc: 0.8870\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.2714 - acc: 0.8945 - val_loss: 0.2799 - val_acc: 0.8870\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.2701 - acc: 0.8955 - val_loss: 0.2802 - val_acc: 0.8860\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.2681 - acc: 0.8970 - val_loss: 0.2781 - val_acc: 0.8890\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.2672 - acc: 0.8970 - val_loss: 0.2780 - val_acc: 0.8890\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.2655 - acc: 0.8995 - val_loss: 0.2767 - val_acc: 0.8890\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.2641 - acc: 0.8965 - val_loss: 0.2755 - val_acc: 0.8890\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.2627 - acc: 0.8975 - val_loss: 0.2744 - val_acc: 0.8890\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.2617 - acc: 0.8980 - val_loss: 0.2754 - val_acc: 0.8880\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.2602 - acc: 0.8995 - val_loss: 0.2732 - val_acc: 0.8900\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.2590 - acc: 0.8980 - val_loss: 0.2743 - val_acc: 0.8870\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.2578 - acc: 0.9000 - val_loss: 0.2709 - val_acc: 0.8890\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.2564 - acc: 0.9005 - val_loss: 0.2709 - val_acc: 0.8900\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.2553 - acc: 0.9005 - val_loss: 0.2695 - val_acc: 0.8880\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2537 - acc: 0.9020 - val_loss: 0.2681 - val_acc: 0.8930\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.2531 - acc: 0.8985 - val_loss: 0.2694 - val_acc: 0.8900\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.2519 - acc: 0.9025 - val_loss: 0.2686 - val_acc: 0.8910\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.2505 - acc: 0.9020 - val_loss: 0.2665 - val_acc: 0.8890\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.2496 - acc: 0.9025 - val_loss: 0.2686 - val_acc: 0.8920\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.2484 - acc: 0.8995 - val_loss: 0.2654 - val_acc: 0.8890\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2475 - acc: 0.9015 - val_loss: 0.2678 - val_acc: 0.8930\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.2466 - acc: 0.9010 - val_loss: 0.2652 - val_acc: 0.8910\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.2456 - acc: 0.9030 - val_loss: 0.2641 - val_acc: 0.8910\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.2444 - acc: 0.9025 - val_loss: 0.2632 - val_acc: 0.8900\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 0.2436 - acc: 0.9035 - val_loss: 0.2625 - val_acc: 0.8900\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.2428 - acc: 0.9055 - val_loss: 0.2635 - val_acc: 0.8910\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.2413 - acc: 0.9035 - val_loss: 0.2644 - val_acc: 0.8940\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.2404 - acc: 0.9035 - val_loss: 0.2634 - val_acc: 0.8930\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2399 - acc: 0.9050 - val_loss: 0.2616 - val_acc: 0.8910\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.2390 - acc: 0.9070 - val_loss: 0.2610 - val_acc: 0.8920\n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 0.2383 - acc: 0.9050 - val_loss: 0.2606 - val_acc: 0.8920\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 0.2374 - acc: 0.9065 - val_loss: 0.2631 - val_acc: 0.8940\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2365 - acc: 0.9055 - val_loss: 0.2622 - val_acc: 0.8940\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2357 - acc: 0.9050 - val_loss: 0.2608 - val_acc: 0.8930\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.2352 - acc: 0.9075 - val_loss: 0.2606 - val_acc: 0.8940\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.2340 - acc: 0.9085 - val_loss: 0.2585 - val_acc: 0.8920\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.2333 - acc: 0.9060 - val_loss: 0.2593 - val_acc: 0.8950\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.2324 - acc: 0.9090 - val_loss: 0.2581 - val_acc: 0.8920\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 0.2318 - acc: 0.9085 - val_loss: 0.2588 - val_acc: 0.8950\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.2311 - acc: 0.9095 - val_loss: 0.2566 - val_acc: 0.8950\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2302 - acc: 0.9075 - val_loss: 0.2565 - val_acc: 0.8950\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2296 - acc: 0.9085 - val_loss: 0.2581 - val_acc: 0.8950\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 0.2289 - acc: 0.9090 - val_loss: 0.2564 - val_acc: 0.8970\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.2280 - acc: 0.9105 - val_loss: 0.2569 - val_acc: 0.8940\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.2274 - acc: 0.9085 - val_loss: 0.2569 - val_acc: 0.8950\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.2264 - acc: 0.9090 - val_loss: 0.2595 - val_acc: 0.8920\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.2258 - acc: 0.9110 - val_loss: 0.2544 - val_acc: 0.8920\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.2254 - acc: 0.9095 - val_loss: 0.2551 - val_acc: 0.8970\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.2247 - acc: 0.9105 - val_loss: 0.2543 - val_acc: 0.8950\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.2239 - acc: 0.9095 - val_loss: 0.2548 - val_acc: 0.8960\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 0s 121us/step - loss: 0.2233 - acc: 0.9115 - val_loss: 0.2547 - val_acc: 0.8960\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim= 512))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=100,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
