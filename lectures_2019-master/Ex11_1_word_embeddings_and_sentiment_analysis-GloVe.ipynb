{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 11-1. Word Embeddings with GloVe and Sentiment Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe\n",
    "    - window 안에 속하는 단어들만을 반영하는 word2vec의 단점을 해결하기 위한 아이디어\n",
    "    - 전체 dictionary에서 두 단어의 동시등장(co-occurrence)하는 확률을 계산하고 동시에 등장하는 확률이 높을 수록 두 단어 벡터가 가까워지도록 학습\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.'],\n",
    "              ['i', 'liked', 'the', 'da', 'vinci', 'code', 'a', 'lot', '.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터는 각 문장을 단어들의 list로 표현하여 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() \n",
    "corpus.fit(input_text, window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Co-occurrence를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Glove(no_components=5)\n",
    "glove.fit(corpus.matrix, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 생성한 co-occurrence matrix를 입력값으로 받아 glove 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.word_vectors # embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서를 line별로 읽어들이면서 단어의 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_FEATURES = 2000   \n",
    "MAX_SENTENCE_LENGTH = 40  \n",
    "\n",
    "import collections\n",
    "import os \n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)  # the maximum number of words in a sentence\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1  # frequency for each word\n",
    "    num_recs += 1 # total number of records\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 등장 빈도를 기준으로 `MAX_FEATURES` 만큼의 단어를 vocabulary로 결정\n",
    "- vocabulary에 속하지 않는 단어는 \"UNK\"로 표시하면서 문장을 단어 단위로 tokenize 하고 list로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:03:10.031128Z",
     "start_time": "2018-07-10T17:03:09.018550Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = [v for v, _ in word_freqs.most_common(MAX_FEATURES)]\n",
    "\n",
    "sentences = np.empty((num_recs, ), dtype=list)\n",
    "i = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    sentence = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            sentence.append(word)\n",
    "        else:\n",
    "            sentence.append(\"UNK\")\n",
    "    sentences[i] = sentence\n",
    "    i += 1\n",
    "    \n",
    "ftrain.close()\n",
    "\n",
    "sentences=list(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: GloVe 알고리즘을 통해 word embedding을 하고 embedding matrix를 `embedding_matrix_glove`의 이름으로 저장하시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "# Your answer comes here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: Word2vec 알고리즘을 사용하는 Lecture 11의 예제와 동일하게 이후 과정 진행\n",
    "\n",
    "- Embedding matrix에 \"UNK\"을 나타내는 0 행을 추가 \n",
    "- Look-up dictionary 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_glove =                 \n",
    "\n",
    "\n",
    "index2word = \n",
    "\n",
    "\n",
    "word2index = \n",
    "\n",
    "\n",
    "vocab_size = len(index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras embedding layer에 입력하기 위해 단어 인덱스를 사용하여 문장을 list로 변환하여 저장하고 각 문장의 sentiment label 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: \n",
    "\n",
    "GloVe에 의해 학습된 embedding matrix를 사용하여 word2vec를 사용해 학습했던 모형과 동일한 구조의 모형을 학습하고 test set에 대한 accuracy를 계산하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Your answer comes here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
