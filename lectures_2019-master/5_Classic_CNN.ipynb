{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN을 구성하는 layers\n",
    "- Input layer\n",
    "    - 입력 이미지가 가로32, 세로32, 그리고 RGB 채널을 가지는 경우 입력의 크기는 [32x32x3]\n",
    "- Convolution layer\n",
    "    - 각 convolution operation은 filter를 통해 input의 일부분과 연결\n",
    "    - 만일 12개의 filter를 사용한다면 [32(?)x32(?)x12] 크기의 출력 생성\n",
    "    - 일반적으로 ReLU activation과 함께 사용\n",
    "- Pooling layer\n",
    "    - downsampling을 통해 계산량을 줄이고 특성을 강화\n",
    "    - [32x32x12]를 입력받았다면 [16x16x12]를 출력\n",
    "    - 학습할 parameter가 없음 \n",
    "    ![](figures/pooling2.PNG)\n",
    "\n",
    "- Fully-connected layer\n",
    "    - Input의 각 뉴런들과 모두 연결되어 있는 레이어\n",
    "    - 만일 노드가 10개라면 [10x1] 크기를 출력 \n",
    "    - 일반적으로 ReLU activation과 함께 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"700\"\n",
       "            src=\"https://cs231n.github.io/assets/conv-demo/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcacc5cd2e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='https://cs231n.github.io/assets/conv-demo/index.html', width=800, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: $ W_1 \\times H_1 \\times D_1$ 크기를 입력으로 받음 \n",
    "- Hyperparameters\n",
    "    - Filter의 수: $K$\n",
    "    - Filter의 크기: $F$\n",
    "    - Stride(몇 칸씩 건너뛰며 convolution operation을 하는가?): $S$\n",
    "    - Zero padding의 수: $P$ \n",
    "- Output: $ W_2 \\times H_2 \\times D_2$ 크기가 output으로 나옴\n",
    "    - $W_2 = (W_1 - F+2P)/S + 1$\n",
    "    - $H_2 = (H_1 - F+2P)/S+1$\n",
    "    - $D_2=K$\n",
    "\n",
    "### Covolution architecture의 패턴\n",
    "일반적으로 convolution(CONV), pooling(POOL), fully connected(FC) layer로 구성되어 있음\n",
    "- <span style=\"background-color:lightgreen\">__INPUT -> FC__</span>\n",
    "    - 선형 분류기\n",
    "- <span style=\"background-color:lightgreen\">__INPUT -> CONV -> FC__</span>\n",
    "- <span style=\"background-color:lightgreen\">__INPUT -> [CONV -> POOL]*2 -> FC -> FC__</span>\n",
    "    - CONV 하나와 POOL 하나를 연결하는 세트를 두 번 반복 \n",
    "    - FC 하나 연결 후 output layer로 연결\n",
    "- <span style=\"background-color:lightgreen\">__INPUT -> [CONV -> CONV -> POOL]*3  -> FC *2 -> FC__</span>\n",
    "    - CONV 두 개와 POOL 하나를 연결하는 세트를 세 번 반복 \n",
    "    - POOL 연산으로 정보가 손실되기 전에 여러 층으로 쌓인 CONV 레이어를 통해 복잡한 feature들을 추출\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5. Classic Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LeNet (1998)\n",
    "- [Yann LeCunn et al.](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)(NYU)에 의해 개발\n",
    "- CNN 개념의 시초\n",
    "- Handwritten digit recognition (MNIST)에 널리 사용 \n",
    "- Convolution을 거치면서 작은 크기의 feature에서 보다 일반화된 feature를 얻어가는 과정을 제시\n",
    "- Architecture\n",
    "    <img src=\"figures/lenet.PNG\" width=\"80%\">\n",
    "    <img src=\"figures/lenet2.PNG\" width=\"50%\">\n",
    "    \n",
    "    - 약 60,000개의 parameter \n",
    "    - $28\\times28$ 크기인 MNIST input을 padding=2를 사용하여 $32\\times 32$로 만들어 input 이미지 구성\n",
    "    - Average pooling 사용. 현재는 max pooling을 더 많이 사용 \n",
    "    - Output layer에서 radial basis function을 사용. 현재는 softmax를 더 많이 사용 \n",
    "\n",
    "\n",
    "<img src=\"figures/lenet_sample.gif\" width=\"30%\">\n",
    "<center> <small> _http://yann.lecun.com/exdb/lenet/index.html_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet (2012)\n",
    "- Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton(U of Toronto)에 의해 개발 http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "- 2012 ImageNet ILSVRC challenge에서 우승 \n",
    "- 1000개의 범주를 가지는 1500만 개의 image classification\n",
    "<img src=\"figures/alexnet3.PNG\" width=\"60%\">\n",
    "\n",
    "- Architecture\n",
    "\n",
    "    <img src=\"figures/alexnet4.PNG\" width=\"70%\">\n",
    "\n",
    "    - LeNet과 비슷하지만 훨씬 깊고 큰 network \n",
    "    - 5개의 convolution layer, 3개의 fully-connected layer 사용\n",
    "    - GPU 두 개를 기반으로한 병렬 구조\n",
    "    - ReLU activation을 사용\n",
    "    - Dropout의 소개 \n",
    "    - 약 6천만 개의 parameter \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet (2014)\n",
    "- ImageNet ILSVRC Challenge 2014에서 Simonyan and Zisserman(Oxford Univ.)에 의해 제안(2등) http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "- 단순한 구조로 지금까지 많이 사용, 망의 구조보다는 망의 깊이가 중요하다는 것을 보여줌 (16개 layer 이상이면 큰 개선 없음; VGG16이 가장 좋은 성능)\n",
    "- 항상 $3 \\times 3$ filter, Stride=1, same padding, $2\\times 2$ pooling 사용\n",
    "- 2014년 1등인 GoogLeNet 보다 이미지 분류 성능은 낮지만 transfer learning 과제에서 더 좋은 성능을 보이는 경우가 많음 \n",
    "- Architecture \n",
    "    <img src=\"figures/vgg16.PNG\" width=\"60%\">\n",
    "   \n",
    "   - Filter의 수가 64, 128, 256, ... 두 배씩 커짐 \n",
    "   - 약 1억3천8백만 개의 parameter: 매우 많은 메모리와 연산량이 필요\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet(2014)\n",
    "- ImageNet ILSVRC Challenge 2014의 우승 모델. Google의  Szegedy et al.이 제안\n",
    "- Parameter의 수를 획기적으로 줄여주는 inception module을 소개 \n",
    "\n",
    "#### Inception module \n",
    "- 어느 사이즈의 필터를 사용할 것인가? \n",
    "    - $1 \\times 1, 3 \\times 3, 5 \\times 5$ filter를 모두 사용하여 쌓고 weight를 학습시킴\n",
    "    <img src=\"figures/inception1.PNG\" width=\"50%\">\n",
    "- Bottleneck layer \n",
    "    - 계산량이 너무 많은 단점을 해결하기 위해 $1 \\times 1$ filter를 사용하여 사이즈를 줄여준 뒤 $5 \\times 5$ filter 적용  \n",
    "    <img src=\"figures/bottleneck.PNG\" width=\"50%\">\n",
    "- Inception module\n",
    "    - ($1 \\times 1$, $3 \\times 3$),($1 \\times 1$, $5 \\times 5$)를 사용하여 stack \n",
    "<img src=\"figures/inception2.jpg\" width=\"40%\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoogLeNet architecture\n",
    "- Inception module의 반복적용\n",
    "- 중간에 maxpooling으로 사이즈 축소 \n",
    "- Hidden layer에서 softmax를 통해 output을 예측하는 과정을 추가하여 layer 전체를 통과하지 않더라도 나쁘지 않은 예측을 가능하도록 설계(overfitting 방지의 역할)\n",
    "<img src=\"figures/googlenet.png\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet (2015)\n",
    "- ImageNet ILSVRC 2015 우승 (error rate 3.6%, 인간의 error rate 5~10%)\n",
    "- Kaiming He (Microsoft 북경 연구소)\n",
    "- Residual block \n",
    "    <img src=\"figures/resnet.PNG\" width=\"30%\">\n",
    "    \n",
    "    - ReLU를 통해 전달되는 비선형 경로와 레이어를 하나 혹은 두 단계씩 건너뛰는 skip connection(or shortcut)을 결합\n",
    "    <img src=\"figures/resnet2.png\" width=\"30%\">\n",
    "\n",
    "- 이론적으로는 network가 깊어질수록 training set에 대해서는 error가 줄어들어야 하지만 training이 어려워지기 때문에 error가 상승할 수 있음 \n",
    "- Vanishing/exploding gradient problem 발생 \n",
    "- Skip connection을 사용함으로써 얕은 모델 보다 높은 training error를 가질 수 없음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras가 제공하는 ImageNet classification model\n",
    "- 각 유명 모형과 학습된 weight를 제공 \n",
    "https://keras.io/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "References\n",
    "- https://www.coursera.org/specializations/deep-learning\n",
    "- [Hands on Machine Learning with Scikit-Learn  and Tensorflow, Aurélien Géron]( http://www.hanbit.co.kr/store/books/look.php?p_code=B9267655530)\n",
    "- [Deep Learning with Python, François Chollet,](https://www.manning.com/books/deep-learning-with-python)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
