{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "# Lecture 4. Introduction to Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Overview\n",
    "#### 4.1.1 Computer vision problem\n",
    "\n",
    "-  Image classification\n",
    "    - 64\\*64 이미지를 입력하였을 때 고양이인지 여부 판단 \n",
    "    ![](http://cs231n.github.io/assets/classify.png)\n",
    "- Object detection\n",
    "    - 이미지 안의 물체를 탐색하여 자동차에 네모 박스\n",
    "    <img src=\"figures/object.png\" width=\"40%\">\n",
    "- Neural style transfer\n",
    "    - 입력 이미지와 스타일 이미지를 합쳐 새로운 이미지 생성 \n",
    "    <img src=\"figures/style.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep learning with image\n",
    "- 이미지를 input으로 사용하면 dimension이 매우 큼 \n",
    "    - 64\\*64 color image: 12288 input units\n",
    "    - 1000\\*1000 color image: 3,000,000 input units\n",
    "- Hidden unit의 수에 따라 weight의 수가 지나치게 커지기 때문에 메모리 부족 \n",
    "- Fully connected layer(dense layer)만을  사용한다면 이미지의 공간적 구조 학습이 어려움  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Overview of convolution architecture\n",
    "- Architecture of DNN\n",
    "![](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n",
    "- Architecture of CNN\n",
    "    - 각 layer는 3D input을 3D output으로 변환함\n",
    "    - 빨간색 input layer는 input image(Red, Green, Blue channels)\n",
    "    - 각 layer는 convolution operation을 통해 다음 layer로 변환됨\n",
    "![](http://cs231n.github.io/assets/cnn/cnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 Convolution operation\n",
    "#### 4.2.1 Convolution operation 이란? \n",
    "- 이미지를 작은 2D window를 통해 입력하여 local pattern을 학습하는 과정\n",
    "    <img src=\"figures/conv1.PNG\" width=\"30%\">\n",
    "- Convolution operation의 예\n",
    "    - Blue grid: 5*5 input feature map\n",
    "    - 움직이는 matrix \n",
    "    $\\left( \n",
    "    \\begin{matrix}\n",
    "    0&1&2\\\\\n",
    "    2&2&0\\\\\n",
    "    0&1&2\n",
    "    \\end{matrix}\\right)$: 3*3 kernel\n",
    "    - Green grid: 3*3 output feature map\n",
    "- Convolution operation을 통한  edge detection\n",
    "    <img src=\"figures/edge.PNG\" width=\"40%\">\n",
    "    - 특정 kernel을 사용한 output이 수직 가장자리를 감지 \n",
    "    <img src=\"figures/conv.PNG\" width=\"50%\">\n",
    "    \n",
    "    - CNN에서는 주어진 kernel의 값을 적용하지 않고 kernel의 값을 weight로 모형을 통해 학습\n",
    "        <img src=\"figures/numerical_no_padding_no_strides.gif\" width=\"50%\">\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Convolution operation의 특징 \n",
    "\n",
    "\n",
    "- Translation invariant\n",
    "    - 오른쪽 아래 모서리에서 학습된 패턴을 다른 곳에서도 인식할 수 있음\n",
    "    - 일반화 능력을 가진 표현을 학습 \n",
    "- Learning spatial hierarchies of patterns\n",
    "    - 첫 번째 layer는 edge와 같은 작은 지역 패턴을 학습 \n",
    "    - 두 번째 layer는 첫 번째 층의 특성으로 구성된 더 큰 패턴 학습 (추상적인 시각적 개념을 학습)\n",
    "    <img src=\"figures/conv2.PNG\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Convolutional operation의 작동방식 \n",
    "- 두 개의 parameter로 정의\n",
    "    - Filter의 크기: 일반적으로 3*3, 5*5 크기의 filter를 주로 사용\n",
    "    - Filter의 수: Feature map output의 깊이 \n",
    "\n",
    "- 컬러 이미지는 RGB의 각 이미지로 구성되어 3개의 feature map으로 구성 \n",
    "- 예)\n",
    "    - Input image는  $6 \\times 6 \\times 3$ 형태의 volume\n",
    "    - Filter: $3 \\times 3 \\times 3$ 크기 1개\n",
    "    - Output: $4 \\times 4$  feature map 1개 생성 \n",
    "    <img src=\"figures/conv_rgb.PNG\" width=\"70%\">\n",
    "\n",
    "- 예) \n",
    "    - Input image는  $6 \\times 6 \\times 3$ 형태의 volume\n",
    "    - Filter: $3 \\times 3 \\times 3$ 크기 2개\n",
    "    - Output: $4 \\times 4 $  feature map 2개 생성 \n",
    "     \n",
    "<img src=\"figures/conv_layer.PNG\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Padding\n",
    "\n",
    "- 이미지 가장자리의 픽셀은 convolution 계산에 상대적으로 적게 반영 \n",
    "- 이미지 가장자리를 0으로 둘러싸서 가장자리 픽셀에 대한 반영 횟수를 늘림\n",
    "- \"Valid\" padding\n",
    "    - Padding을 적용하지 않음 \n",
    "- \"Same\" padding \n",
    "    - Input과 output의 이미지 크기가 동일하게 되도록 padding 수를 결정 \n",
    "\n",
    "\n",
    "<img src=\"figures/zero_padding.png\" width=\"30%\" align=\"left\">\n",
    "<img src=\"figures/same_padding_no_strides.gif\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 Strides\n",
    "- Kernel의 이동을 한 번에 몇 칸씩 이동하는가 \n",
    "- Stride=2: 한 번에 두 칸씩 이동 (feature map의 너비와 높이가 2배수로 다운샘플링 되었음을 의미) \n",
    "\n",
    "<img src=\"figures/no_padding_2_strides.gif\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.6 Max-pooling layer\n",
    "- Filter를 통하지 않고 해당 영역의 input 중 가장 큰 값을 출력\n",
    "- 일반적으로 2*2 크기의 window, stride=2 사용 \n",
    "- 강제적인 subsampling 효과 \n",
    "    - weight 수를 줄여 계산속도를 높임\n",
    "    - 특징을 견고하게 감지 \n",
    "- 학습할 weight가 없음: 일반적으로 convolutional layer+pooling layer를 하나의 레이어로 취급 \n",
    "\n",
    "<img src=\"figures/pooling.PNG\" width=\"40%\">\n",
    "<center> *Max pooling layer(2 × 2 pooling kernel, stride 2, no padding)* \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "####  참고\n",
    "- W: input volume size\n",
    "- F: filter size\n",
    "- S: strides\n",
    "- P: the number of zero-paddings\n",
    "\n",
    "$$\\mbox{output neuron의 수:} \\frac{W−F+2P}{S}+1$$\n",
    "\n",
    "- Ex)  7x7 input, 3x3 filter with stride 1 and pad 0 : 5x5 output\n",
    "- Ex) 10x10 input, 3x3 filter with stride 2 and pad 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Example of CNN architecture\n",
    "<img src=\"figures/cnn.PNG\" width=\"80%\">\n",
    "\n",
    "![](http://cs231n.github.io/assets/cnn/convnet.jpeg)\n",
    "- 일반적으로 convolutional layer (+ ReLU or other activations) + pooling layer를 여러 개 쌓음 \n",
    "- Network가 진행될 수록 feature map의 크기는 줄어들고 깊이는 증가\n",
    "- 마지막에 Fully connected layer(+ ReLU or other activations) 추가 \n",
    "- Output 형태에 맞는 output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 CNN structure with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input shape: `(image_height, image_width, image_channels)`\n",
    "\n",
    "- `keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', activation=None)` https://keras.io/layers/convolutional/\n",
    "    - `filters`:  the dimensionality of the output space (i.e. the number of output filters)\n",
    "    - `kernel_size`:  height and width of the 2D convolution window\n",
    "    - `strides`: the strides of the convolution along the height and width\n",
    "    - `padding`: \"valid\" or \"same\"\n",
    "    - `activation`: activation function https://keras.io/activations/\n",
    "- `keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')`\n",
    "    - `pool_size`: Pooling window size\n",
    "    - `strides`: default = `pool_size`\n",
    "- `keras.layers.Flatten()`\n",
    "    - Flattens the input\n",
    "    - Fully connected layer를 적용하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T14:01:25.516814Z",
     "start_time": "2019-02-19T14:01:24.060108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 123,690\n",
      "Trainable params: 123,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- conv2d_1: 3*3 크기의 filter 32개 (288개 weight) + bias 32개 = 320 parameters\n",
    "- max_pooling2d_1: 학습할 parameter 없음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "    TO DO: 나머지 layer의 parameter의 수를 계산하고 확인하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "References\n",
    "- https://www.coursera.org/specializations/deep-learning\n",
    "- [Hands on Machine Learning with Scikit-Learn  and Tensorflow, Aurélien Géron]( http://www.hanbit.co.kr/store/books/look.php?p_code=B9267655530)\n",
    "- [Deep Learning with Python, François Chollet,](https://www.manning.com/books/deep-learning-with-python)\n",
    "- http://cs231n.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2.7 (NGC/DIGITS 18.12-tensorflow) on Backend.AI",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
