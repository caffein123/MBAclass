{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 11-1. Word Embeddings with GloVe and Sentiment Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe\n",
    "    - window 안에 속하는 단어들만을 반영하는 word2vec의 단점을 해결하기 위한 아이디어\n",
    "    - 전체 dictionary에서 두 단어의 동시등장(co-occurrence)하는 확률을 계산하고 동시에 등장하는 확률이 높을 수록 두 단어 벡터가 가까워지도록 학습\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove_python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
      "\u001b[K    100% |████████████████████████████████| 266kB 11.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (from glove_python) (1.14.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.5/dist-packages (from glove_python) (1.1.0)\n",
      "Building wheels for collected packages: glove-python\n",
      "  Building wheel for glove-python (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/work/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
      "Successfully built glove-python\n",
      "Installing collected packages: glove-python\n",
      "Successfully installed glove-python-0.1.0\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.'],\n",
    "              ['i', 'liked', 'the', 'da', 'vinci', 'code', 'a', 'lot', '.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터는 각 문장을 단어들의 list로 표현하여 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.'],\n",
       " ['i', 'liked', 'the', 'da', 'vinci', 'code', 'a', 'lot', '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus() \n",
    "corpus.fit(input_text, window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Co-occurrence를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 8,\n",
       " 'a': 11,\n",
       " 'awesome': 7,\n",
       " 'book': 4,\n",
       " 'code': 3,\n",
       " 'da': 1,\n",
       " 'i': 9,\n",
       " 'is': 5,\n",
       " 'just': 6,\n",
       " 'liked': 10,\n",
       " 'lot': 12,\n",
       " 'the': 0,\n",
       " 'vinci': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t2.0\n",
      "  (0, 2)\t0.5\n",
      "  (0, 2)\t0.5\n",
      "  (0, 3)\t0.6666666865348816\n",
      "  (0, 4)\t0.25\n",
      "  (0, 5)\t0.20000000298023224\n",
      "  (0, 6)\t0.1666666716337204\n",
      "  (0, 7)\t0.1428571492433548\n",
      "  (0, 8)\t0.2916666865348816\n",
      "  (0, 9)\t0.5\n",
      "  (0, 10)\t1.0\n",
      "  (0, 11)\t0.25\n",
      "  (0, 12)\t0.20000000298023224\n",
      "  (1, 2)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 4)\t0.3333333432674408\n",
      "  (1, 5)\t0.25\n",
      "  (1, 6)\t0.20000000298023224\n",
      "  (1, 7)\t0.1666666716337204\n",
      "  (1, 8)\t0.34285715222358704\n",
      "  (1, 9)\t0.3333333432674408\n",
      "  (1, 10)\t0.5\n",
      "  (1, 11)\t0.3333333432674408\n",
      "  (1, 12)\t0.25\n",
      "  :\t:\n",
      "  (3, 8)\t0.5333333611488342\n",
      "  (3, 9)\t0.20000000298023224\n",
      "  (3, 10)\t0.25\n",
      "  (3, 11)\t1.0\n",
      "  (3, 12)\t0.5\n",
      "  (4, 5)\t1.0\n",
      "  (4, 6)\t0.5\n",
      "  (4, 7)\t0.3333333432674408\n",
      "  (4, 8)\t0.25\n",
      "  (5, 6)\t1.0\n",
      "  (5, 7)\t0.5\n",
      "  (5, 8)\t0.3333333432674408\n",
      "  (6, 7)\t1.0\n",
      "  (6, 8)\t0.5\n",
      "  (7, 8)\t1.0\n",
      "  (8, 9)\t0.125\n",
      "  (8, 10)\t0.1428571492433548\n",
      "  (8, 11)\t0.5\n",
      "  (8, 12)\t1.0\n",
      "  (9, 10)\t1.0\n",
      "  (9, 11)\t0.1666666716337204\n",
      "  (9, 12)\t0.1428571492433548\n",
      "  (10, 11)\t0.20000000298023224\n",
      "  (10, 12)\t0.1666666716337204\n",
      "  (11, 12)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(corpus.matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Glove(no_components=5)\n",
    "glove.fit(corpus.matrix, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 생성한 co-occurrence matrix를 입력값으로 받아 glove 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00571506, -0.07490707,  0.06170742, -0.05462285, -0.06623276],\n",
       "       [-0.09634959,  0.02058038, -0.05428312,  0.076029  ,  0.05931962],\n",
       "       [-0.09359689,  0.08001433,  0.05762888, -0.0484546 ,  0.06456515],\n",
       "       [-0.03261592,  0.0912928 ,  0.05551798, -0.06455152,  0.05396033],\n",
       "       [ 0.0111836 ,  0.08125799,  0.08738593,  0.03845196, -0.02604924],\n",
       "       [ 0.09827774, -0.00585431, -0.00523   ,  0.00721282,  0.08182504],\n",
       "       [-0.05834477,  0.01547166,  0.03455233, -0.08697362,  0.04096139],\n",
       "       [-0.0658658 , -0.01118439,  0.08704402, -0.03216629, -0.03447983],\n",
       "       [ 0.03785402, -0.08968755, -0.08456509, -0.01508005, -0.05850994],\n",
       "       [ 0.01515708,  0.06430392,  0.08989079, -0.02950438, -0.07062391],\n",
       "       [-0.05711068, -0.05089665,  0.03456115, -0.06364022, -0.01460702],\n",
       "       [-0.00610676,  0.03511082,  0.01297673, -0.01749829, -0.00386495],\n",
       "       [ 0.04157727,  0.09334284, -0.06312205, -0.08330263, -0.07715888]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.word_vectors # embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서를 line별로 읽어들이면서 단어의 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/work/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_FEATURES = 2000   \n",
    "MAX_SENTENCE_LENGTH = 40  \n",
    "\n",
    "import collections\n",
    "import os \n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)  # the maximum number of words in a sentence\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1  # frequency for each word\n",
    "    num_recs += 1 # total number of records\n",
    "ftrain.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 등장 빈도를 기준으로 `MAX_FEATURES` 만큼의 단어를 vocabulary로 결정\n",
    "- vocabulary에 속하지 않는 단어는 \"UNK\"로 표시하면서 문장을 단어 단위로 tokenize 하고 list로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T17:03:10.031128Z",
     "start_time": "2018-07-10T17:03:09.018550Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = [v for v, _ in word_freqs.most_common(MAX_FEATURES)]\n",
    "\n",
    "sentences = np.empty((num_recs, ), dtype=list)\n",
    "i = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    sentence = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            sentence.append(word)\n",
    "        else:\n",
    "            sentence.append(\"UNK\")\n",
    "    sentences[i] = sentence\n",
    "    i += 1\n",
    "    \n",
    "ftrain.close()\n",
    "\n",
    "sentences=list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7086"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'da', 'vinci', 'code', 'book', 'is', 'just', 'awesome', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: GloVe 알고리즘을 통해 word embedding을 하고 embedding matrix를 `embedding_matrix_glove`의 이름으로 저장하시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "corpus = Corpus() \n",
    "corpus.fit(sentences, window=10)\n",
    "glove = Glove(no_components=EMBEDDING_SIZE)\n",
    "glove.fit(corpus.matrix, epochs=30)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_glove = glove.word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002, 128)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: Word2vec 알고리즘을 사용하는 Lecture 11의 예제와 동일하게 이후 과정 진행\n",
    "\n",
    "- Embedding matrix에 \"UNK\"을 나타내는 0 행을 추가 \n",
    "- Look-up dictionary 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_glove = np.append(np.zeros((1,EMBEDDING_SIZE)), embedding_matrix_glove, axis=0)\n",
    "\n",
    "index2word = {i+1: w for i, w in enumerate(glove.dictionary)} \n",
    "index2word[0] = 'PAD'\n",
    "word2index = {w: i for i, w in index2word.items() }\n",
    "\n",
    "vocab_size = len(index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras embedding layer에 입력하기 위해 단어 인덱스를 사용하여 문장을 list로 변환하여 저장하고 각 문장의 sentiment label 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(\"data/umich-sentiment-train.txt\", 'rb')\n",
    "\n",
    "for line in ftrain:\n",
    "    label, sentence = line.decode('utf8').strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0, 1189, 1761,\n",
       "        373,  456,  233,  497, 1361, 1843,  406], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5668, 40) (1418, 40) (5668,) (1418,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "TO DO: \n",
    "\n",
    "GloVe에 의해 학습된 embedding matrix를 사용하여 word2vec를 사용해 학습했던 모형과 동일한 구조의 모형을 학습하고 test set에 대한 accuracy를 계산하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 128)           256256    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 276,897\n",
      "Trainable params: 20,641\n",
      "Non-trainable params: 256,256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length = MAX_SENTENCE_LENGTH, mask_zero = True,\n",
    "                    weights = [embedding_matrix_glove], trainable = False))\n",
    "model.add(LSTM(32, recurrent_dropout = 0.2, return_sequences = False))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/100\n",
      "5668/5668 [==============================] - 2s 330us/step - loss: 0.6793 - acc: 0.5734 - val_loss: 0.6686 - val_acc: 0.5818\n",
      "Epoch 2/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.6623 - acc: 0.6000 - val_loss: 0.6589 - val_acc: 0.6255\n",
      "Epoch 3/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.6506 - acc: 0.6332 - val_loss: 0.6437 - val_acc: 0.6347\n",
      "Epoch 4/100\n",
      "5668/5668 [==============================] - 1s 185us/step - loss: 0.6315 - acc: 0.6413 - val_loss: 0.6187 - val_acc: 0.6368\n",
      "Epoch 5/100\n",
      "5668/5668 [==============================] - 1s 184us/step - loss: 0.6049 - acc: 0.6431 - val_loss: 0.5939 - val_acc: 0.6537\n",
      "Epoch 6/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.5875 - acc: 0.6581 - val_loss: 0.5816 - val_acc: 0.6784\n",
      "Epoch 7/100\n",
      "5668/5668 [==============================] - 1s 196us/step - loss: 0.5725 - acc: 0.6870 - val_loss: 0.5598 - val_acc: 0.6932\n",
      "Epoch 8/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.5548 - acc: 0.7048 - val_loss: 0.5367 - val_acc: 0.7200\n",
      "Epoch 9/100\n",
      "5668/5668 [==============================] - 1s 192us/step - loss: 0.5392 - acc: 0.7262 - val_loss: 0.5170 - val_acc: 0.7405\n",
      "Epoch 10/100\n",
      "5668/5668 [==============================] - 1s 184us/step - loss: 0.5175 - acc: 0.7511 - val_loss: 0.4917 - val_acc: 0.7793\n",
      "Epoch 11/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.4982 - acc: 0.7616 - val_loss: 0.4694 - val_acc: 0.7969\n",
      "Epoch 12/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.4801 - acc: 0.7839 - val_loss: 0.4524 - val_acc: 0.8068\n",
      "Epoch 13/100\n",
      "5668/5668 [==============================] - 1s 190us/step - loss: 0.4700 - acc: 0.7906 - val_loss: 0.4399 - val_acc: 0.8096\n",
      "Epoch 14/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.4565 - acc: 0.8038 - val_loss: 0.4279 - val_acc: 0.8202\n",
      "Epoch 15/100\n",
      "5668/5668 [==============================] - 1s 192us/step - loss: 0.4450 - acc: 0.8080 - val_loss: 0.4186 - val_acc: 0.8209\n",
      "Epoch 16/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.4318 - acc: 0.8102 - val_loss: 0.4126 - val_acc: 0.8209\n",
      "Epoch 17/100\n",
      "5668/5668 [==============================] - 1s 193us/step - loss: 0.4270 - acc: 0.8118 - val_loss: 0.4048 - val_acc: 0.8343\n",
      "Epoch 18/100\n",
      "5668/5668 [==============================] - 1s 192us/step - loss: 0.4173 - acc: 0.8160 - val_loss: 0.3988 - val_acc: 0.8188\n",
      "Epoch 19/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.4097 - acc: 0.8176 - val_loss: 0.3936 - val_acc: 0.8350\n",
      "Epoch 20/100\n",
      "5668/5668 [==============================] - 1s 193us/step - loss: 0.4071 - acc: 0.8285 - val_loss: 0.3885 - val_acc: 0.8336\n",
      "Epoch 21/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.3961 - acc: 0.8229 - val_loss: 0.3817 - val_acc: 0.8265\n",
      "Epoch 22/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3922 - acc: 0.8269 - val_loss: 0.3783 - val_acc: 0.8449\n",
      "Epoch 23/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.3868 - acc: 0.8310 - val_loss: 0.3784 - val_acc: 0.8413\n",
      "Epoch 24/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.3850 - acc: 0.8331 - val_loss: 0.3752 - val_acc: 0.8286\n",
      "Epoch 25/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.3775 - acc: 0.8340 - val_loss: 0.3718 - val_acc: 0.8357\n",
      "Epoch 26/100\n",
      "5668/5668 [==============================] - 1s 190us/step - loss: 0.3776 - acc: 0.8361 - val_loss: 0.3686 - val_acc: 0.8413\n",
      "Epoch 27/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3690 - acc: 0.8412 - val_loss: 0.3664 - val_acc: 0.8449\n",
      "Epoch 28/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3684 - acc: 0.8396 - val_loss: 0.3616 - val_acc: 0.8420\n",
      "Epoch 29/100\n",
      "5668/5668 [==============================] - 1s 194us/step - loss: 0.3637 - acc: 0.8410 - val_loss: 0.3604 - val_acc: 0.8449\n",
      "Epoch 30/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.3620 - acc: 0.8440 - val_loss: 0.3571 - val_acc: 0.8434\n",
      "Epoch 31/100\n",
      "5668/5668 [==============================] - 1s 190us/step - loss: 0.3613 - acc: 0.8472 - val_loss: 0.3555 - val_acc: 0.8660\n",
      "Epoch 32/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.3535 - acc: 0.8536 - val_loss: 0.3537 - val_acc: 0.8420\n",
      "Epoch 33/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3547 - acc: 0.8497 - val_loss: 0.3503 - val_acc: 0.8653\n",
      "Epoch 34/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3571 - acc: 0.8504 - val_loss: 0.3495 - val_acc: 0.8441\n",
      "Epoch 35/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.3464 - acc: 0.8495 - val_loss: 0.3454 - val_acc: 0.8653\n",
      "Epoch 36/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3463 - acc: 0.8514 - val_loss: 0.3450 - val_acc: 0.8632\n",
      "Epoch 37/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.3466 - acc: 0.8566 - val_loss: 0.3449 - val_acc: 0.8519\n",
      "Epoch 38/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3404 - acc: 0.8571 - val_loss: 0.3453 - val_acc: 0.8618\n",
      "Epoch 39/100\n",
      "5668/5668 [==============================] - 1s 185us/step - loss: 0.3427 - acc: 0.8585 - val_loss: 0.3425 - val_acc: 0.8667\n",
      "Epoch 40/100\n",
      "5668/5668 [==============================] - 1s 193us/step - loss: 0.3383 - acc: 0.8604 - val_loss: 0.3402 - val_acc: 0.8646\n",
      "Epoch 41/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.3309 - acc: 0.8649 - val_loss: 0.3393 - val_acc: 0.8646\n",
      "Epoch 42/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.3267 - acc: 0.8626 - val_loss: 0.3369 - val_acc: 0.8639\n",
      "Epoch 43/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.3286 - acc: 0.8654 - val_loss: 0.3401 - val_acc: 0.8505\n",
      "Epoch 44/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.3370 - acc: 0.8566 - val_loss: 0.3355 - val_acc: 0.8625\n",
      "Epoch 45/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.3303 - acc: 0.8682 - val_loss: 0.3367 - val_acc: 0.8646\n",
      "Epoch 46/100\n",
      "5668/5668 [==============================] - 1s 189us/step - loss: 0.3284 - acc: 0.8633 - val_loss: 0.3331 - val_acc: 0.8646\n",
      "Epoch 47/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3233 - acc: 0.8666 - val_loss: 0.3307 - val_acc: 0.8674\n",
      "Epoch 48/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3219 - acc: 0.8666 - val_loss: 0.3282 - val_acc: 0.8674\n",
      "Epoch 49/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.3285 - acc: 0.8626 - val_loss: 0.3287 - val_acc: 0.8667\n",
      "Epoch 50/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.3273 - acc: 0.8641 - val_loss: 0.3323 - val_acc: 0.8632\n",
      "Epoch 51/100\n",
      "5668/5668 [==============================] - 1s 186us/step - loss: 0.3163 - acc: 0.8668 - val_loss: 0.3263 - val_acc: 0.8660\n",
      "Epoch 52/100\n",
      "5668/5668 [==============================] - 1s 185us/step - loss: 0.3191 - acc: 0.8661 - val_loss: 0.3234 - val_acc: 0.8646\n",
      "Epoch 53/100\n",
      "5668/5668 [==============================] - 1s 191us/step - loss: 0.3121 - acc: 0.8712 - val_loss: 0.3229 - val_acc: 0.8625\n",
      "Epoch 54/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.3157 - acc: 0.8682 - val_loss: 0.3216 - val_acc: 0.8660\n",
      "Epoch 55/100\n",
      "5668/5668 [==============================] - 1s 188us/step - loss: 0.3131 - acc: 0.8654 - val_loss: 0.3236 - val_acc: 0.8646\n",
      "Epoch 56/100\n",
      "5668/5668 [==============================] - 1s 184us/step - loss: 0.3118 - acc: 0.8740 - val_loss: 0.3214 - val_acc: 0.8646\n",
      "Epoch 57/100\n",
      "5668/5668 [==============================] - 1s 191us/step - loss: 0.3120 - acc: 0.8716 - val_loss: 0.3239 - val_acc: 0.8667\n",
      "Epoch 58/100\n",
      "5668/5668 [==============================] - 1s 187us/step - loss: 0.3011 - acc: 0.8735 - val_loss: 0.3240 - val_acc: 0.8632\n",
      "Epoch 59/100\n",
      "5668/5668 [==============================] - 1s 183us/step - loss: 0.3070 - acc: 0.8709 - val_loss: 0.3257 - val_acc: 0.8625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2f022b898>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "now = time.strftime(\"%c\")\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(filepath='models/sentiment_analysis_glove.h5', monitor='val_loss', save_best_only=True),\n",
    "    TensorBoard(log_dir='logs/sentiment_analysis_glove/'+now),\n",
    "    EarlyStopping(monitor='val_loss',patience=3)\n",
    "]\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(Xtest, ytest), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418/1418 [==============================] - 0s 74us/step\n",
      "Test loss: 0.326, accuracy: 0.862\n"
     ]
    }
   ],
   "source": [
    "loss_test, acc_test = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test loss: %.3f, accuracy: %.3f\" % (loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\ti love harry potter ...\n",
      "0\t0\ti hate harry potter , it 's retarted , gay and stupid and there 's only one black guy ...\n",
      "0\t0\tby the way , the da vinci code sucked , just letting you know ...\n",
      "1\t1\tbrokeback mountain was an awesome movie .\n",
      "1\t0\ti hate harry potter .\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,40)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
