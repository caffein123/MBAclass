{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "# Lecture 9. Introduction to Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Example of sequence data\n",
    "\n",
    "#### Sequence-to-vector \n",
    "\n",
    "\n",
    "- 예: Sentiment classification \n",
    "    <img src=\"figures/sentiment.png\" width=\"50%\">\n",
    "   \n",
    "    - Input: 단어(또는 문자)를 순차적으로 입력\n",
    "    - Output: 마지막 단어가 입력된 시점에 전체 문장의 감성을 분류\n",
    "     <img src=\"figures/seq2one.PNG\" width=\"30%\">\n",
    "\n",
    "#### Sequence-to-sequence\n",
    "- 예: Music generation \n",
    "    <img src=\"figures/music.PNG\" width=\"50%\">\n",
    "\n",
    "    - Input: 20개의 순차적인 음정, 속도 등의 값 입력\n",
    "    - Output: 이어지는 21번째 음정, 속도 값을 출력 \n",
    "    <img src=\"figures/seq2seq.PNG\" width=\"30%\">\n",
    "\n",
    "\n",
    "#### Delayed sequence-to-sequence\n",
    "- 예: Machine translation\n",
    "    <img src=\"figures/translation.PNG\" width=\"50%\">\n",
    "\n",
    "    - Input: 프랑스어 문장 전체를 단어 단위로 순차적 입력\n",
    "    - Output: 번역된 영어 문장을 단어 단위로 순차적 출력\n",
    "    <img src=\"figures/seq2seq_delay.PNG\" width=\"30%\">\n",
    "\n",
    "\n",
    "#### Vector-to-sequence\n",
    "- 예: Image captioning \n",
    "    <img src=\"figures/imagecaption.png\" width=\"50%\">\n",
    "\n",
    "    - Input: 이미지\n",
    "    - Output: 이미지에 대한 설명을 단어 단위로 순차적으로 출력 \n",
    "    <img src=\"figures/one2seq.PNG\" width=\"30%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Recurrent neural networks\n",
    "### Example\n",
    "- 문장을 입력하여 각 단어가 사람이름을 나타내는지 여부를 구분 \n",
    "- Tokenize\n",
    "    - 문장을 모형의 입력단위인 단어(혹은 음절)로 쪼개는 작업\n",
    "- Input: 단어\n",
    "- Output: 1 if 사람이름, 0 elsewhere\n",
    "\n",
    "\n",
    "<img src=\"figures/entity_exam.PNG\" width=\"50%\">\n",
    "\n",
    "- Vocabulary(dictionary) 구성 \n",
    "    - 데이터 처리 시 사용할 모든 단어의 집합 (예, 10,000개의 단어 집합)\n",
    "    - 각 단어를 $10000\\times 1$ 크기의 one-hot vector로 표현 \n",
    "<img src=\"figures/vocabulary.PNG\" width=\"60%\">\n",
    "<img src=\"figures/vocab2.png\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neuron\n",
    "- 현 시점의 input과 앞 시점의 hidden state를  동시에 input으로 받는 neuron\n",
    "- 앞에 나온 단어가 현재 단어의 사람이름 여부에 영향을 미침\n",
    "\n",
    "<img src=\"figures/recurrent_neuron.PNG\" width=\"40%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_t = \\phi (x_t^T W_x + h_{t-1}^T W_h + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks\n",
    "- Recurrent neuron을 여러 개로 하나의 layer를 구성 \n",
    "<img src=\"figures/recurrent_layer.PNG\" width=\"50%\">\n",
    "\n",
    "- 여러 개의 layer를 쌓아서 stacked RNN을 구성 \n",
    "<img src=\"figures/stacked_recurrent_layer.PNG\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single RNN layer with Keras\n",
    "- `input_shape=(timesteps, input_features)`\n",
    "    - $10000\\times 1$의 one-hot vector로 표현되는 순차적인 3개 단어를 input \n",
    "    - (Harry, Potter, and) -> 0\n",
    "    - (Potter, and, Hermione) -> 1\n",
    "- `return_sequences=False`\n",
    "    - 마지막 시점의 output 만을 출력 \n",
    "    - `(batch_size, output_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T16:14:16.917688Z",
     "start_time": "2018-07-09T16:14:16.794825Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                321056    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 321,089\n",
      "Trainable params: 321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(3,10000), return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_t = \\phi (x_t^T W_x + h_{t-1}^T W_h + b)$$\n",
    "- $W_x$: 10000개의 input 값과 32개 output($h_t$)과의 연결 = $32 \\times 10000 = 320000$\n",
    "- $W_h$: 32개 $h_{t-1}$과 32개 $h_{t}$와의 연결 = $32 \\times 32 = 1024$ \n",
    "- $b$: 32개 bias\n",
    "\n",
    "=> 320000 + 1024 + 32 = 321056"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked RNN layers with Keras\n",
    "\n",
    "- `return_sequences=True`\n",
    "    - 매 시점의 output을 출력 \n",
    "    - `(batch_size, timesteps, output_features)`\n",
    "    - 다음 layer의 recurrent neuron에 입력하기 위해서는 매 시점의 output이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T16:14:29.089335Z",
     "start_time": "2018-07-09T16:14:28.871189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 3, 32)             321056    \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 323,169\n",
      "Trainable params: 323,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(3,10000), return_sequences=True))\n",
    "model.add(SimpleRNN(32, input_shape=(3,10000), return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_t = \\phi (x_t^T W_x + h_{t-1}^T W_h + b)$$\n",
    "- RNN layer 1\n",
    "    - $W_x$: 10000개의 input 값과 32개 output($h_t$)과의 연결 = $32 \\times 10000 = 320000$\n",
    "    - $W_h$: 32개 $h_{t-1}$과 32개 $h_{t}$와의 연결 = $32 \\times 32 = 1024$ \n",
    "    - $b$: 32개 bias\n",
    "    \n",
    "    => 320000 + 1024 + 32 = 321056\n",
    "- RNN layer 2\n",
    "    - $W_x$: 32개의 input 값과 32개 output($h_t$)과의 연결 = $32 \\times 32 = 1024$\n",
    "    - $W_h$: 32개 $h_{t-1}$과 32개 $h_{t}$와의 연결 = $32 \\times 32 = 1024$ \n",
    "    - $b$: 32개 bias\n",
    "    \n",
    "    => 1024 + 1024 + 32 = 2080\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 The Problem of Long-Term Dependencies\n",
    "- 이론적으로 RNN은 예전 입력값을 무한히 기억할 수 있음\n",
    "- 학습과정에서 gradient가 불안정해져서 가중치의 학습이 잘 되지 못함\n",
    "    - Vanishing gradient문제, exploding gradient문제\n",
    "    - 처음의 input값이 점점 잊혀지는 현상 발생 \n",
    "- ReLU activation, parameter initialization의 조정 등 보다 모형의 구조적으로 해결하려는 시도 \n",
    "    - Long Short Term Memory(LSTM; Hochreiter & Schmidhuber, 1997)\n",
    "    - Gated Recurrent Unit(GRU; Kyunghyun Cho et al., 2014) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.1  LSTM cell\n",
    "- 과거 입력값이 무조건 동일한 변환을 통해 전달되는 것이 아니라 필요에 따라 변환 없이 그대로 전달\n",
    "    - f: forget gate, 이전 memory cell값을 얼마나 통과? \n",
    "    - i: input gate, 새로운 정보를 얼마나 통과?\n",
    "    - o: 현재 memory cell 값을 얼마나 외부 네트워크로 통과?\n",
    "    - g: 현재 memory cell의 후보값 \n",
    "    \n",
    "<img src=\"figures/lstm.PNG\" width=\"40%\" align=\"left\">\n",
    "<img src=\"figures/lstm2.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T17:44:28.587278Z",
     "start_time": "2018-07-09T17:44:28.287507Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 32)                1284224   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,284,257\n",
      "Trainable params: 1,284,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(3,10000), return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.2 GRU cell\n",
    "- LSTM cell의 단순화되었지만 성능 좋음\n",
    "- Cell state와 hidden state를 결합\n",
    "- Input gate와 forget gate의 결합\n",
    "\n",
    "<img src=\"figures/gru.PNG\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T17:44:33.716457Z",
     "start_time": "2018-07-09T17:44:33.564684Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(32, input_shape=(3,10000), return_sequences=False))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.3 Bidirectional RNN cell\n",
    "    He said, “Teddy bears are on sale!”\n",
    "    He said, “Teddy Roosevelt was a great President!”\n",
    "- \"Teddy\"가 사람이름을 나타내는지 여부는 앞보다 뒤의 단어들이 영향을 줌\n",
    "- 시간 순서대로 적용한 RNN과 시간 역순대로 적용한 RNN을 합쳐서(concat, ave 등) 구성\n",
    "<img src=\"figures/bidirectional_rnn.PNG\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T17:49:42.572652Z",
     "start_time": "2018-07-09T17:49:41.438179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_4 (Bidirection (None, 3, 64)             2568448   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3, 1)              65        \n",
      "=================================================================\n",
      "Total params: 2,568,513\n",
      "Trainable params: 2,568,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dense\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True),\n",
    "                        input_shape=(3, 10000)))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
