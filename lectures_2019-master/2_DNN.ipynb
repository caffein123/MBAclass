{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=\"darkgreen\">\n",
    "    \n",
    "# Lecture 2. Deep Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.1 What is neural network\n",
    "\n",
    "#### Example 1 – single neural network\n",
    "\n",
    "\n",
    "- 집의 크기로 집 가격을 예측하려고 함 \n",
    "- Linear regression: $price = \\beta_0 + \\beta_1 size + \\epsilon$\n",
    "    - Input: size of house\n",
    "    - Output: price\n",
    "    - 회귀분석에서 $\\beta_0, \\beta_1$는 회귀계수라고 불리지만  neural network에서는 weight($\\beta_1$)와 bias($\\beta_0$)라고 불림\n",
    "    \n",
    "\n",
    "\n",
    "- 집의 가격은 음수가 될 수 없으므로 Rectified Linear Unit (ReLU) 함수를 사용한다면?\n",
    "    - The “neuron” implements the function ReLU (blue line)\n",
    "\n",
    "\n",
    "<img src=\"figures/fig1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 – Multiple neural network\n",
    "- Input이 집의 크기 외에 화장실 수, 우편번호, 지역 소득수준 등 여러 개인 경우 \n",
    "- 바로 output으로 연결되는 것이 아닌 hidden layer를 거쳐서 output으로 연결 되는 경우 \n",
    "\n",
    "<img src=\"figures/fig2.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Neural network representation\n",
    "- 하나의 hidden node의 구성\n",
    "    ![](figures/fig3.png)\n",
    "\n",
    "    - Input vector: $\\mathbb x=(x_1, x_2, x_3)^T$\n",
    "    - Weights: $\\mathbb w = (w_1, w_2, w_3)^T$\n",
    "    - Bias: $ b \\in \\mathbb R$\n",
    "    - Activation function: $\\sigma(\\cdot)$\n",
    "    \n",
    "       <center> $z=w_1 x_1 + w_2 x_2 + w_3 x_3 + b \\Leftrightarrow z = \\mathbb w^T \\mathbb x + b$ <br>\n",
    "       $a=\\sigma( z)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Special case: linear regression     \n",
    "    - $\\hat y = \\sigma(\\mathbb w^T \\mathbb x+b)$  \n",
    "    - $\\sigma(z)=z$\n",
    "    \n",
    "- Special case: logistic regression \n",
    "    - $\\hat y = \\sigma(\\mathbb w^T \\mathbb x+b)$\n",
    "    - $\\sigma(z) = \\frac 1 {1+e^{-z}}$: sigmoid function (inverse-logit function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Deep neural network\n",
    "- Input, hidden, output layer로 구성 \n",
    "- 각 layer는 여러 개의 neuron들로 구성\n",
    "- Deep neural network 구조를 결정하는 요소 \n",
    "    - layer의 개수 \n",
    "    - 한 layer의 neuron(hidden unit)의 개수 \n",
    "    - Activation 함수의 종류\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://stanford.edu/~shervine/images/neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $i$ 번째 layer의 $j$ 번째 hidden unit에 대한 output \n",
    "$$ z_j^{[i]} = {\\mathbb w_j^{[i]}}^T \\mathbb x + b_j ^{[i]} $$\n",
    "$$  a_j^{[i]} = \\sigma( z_j^{[i]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Example\n",
    "![](figures/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Non-deep neural network\n",
    "    - Input layer: input neuron들로 구성. dimension=6\n",
    "    - Hidden layer: 1개 layer, 9개 unit \n",
    "    - Output layer: 4개 unit\n",
    "- Deep neural network \n",
    "    - Input dimension = 8\n",
    "    - Hidden layer: 3개 layer, 각 9개 unit \n",
    "    - Output layer: 4개 unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T07:32:27.735340Z",
     "start_time": "2019-02-13T07:32:27.689458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 9)                 63        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 40        \n",
      "=================================================================\n",
      "Total params: 103\n",
      "Trainable params: 103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(9, input_shape=(6,)))\n",
    "model.add(Dense(4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Layer 1: weight 54개(6*9) +  bias 9개 = 63개\n",
    "- Layer 2: weight 36개(9*4) + bias 4개 = 40개\n",
    "- Total parameters = 63+40 = 103개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T07:30:55.486976Z",
     "start_time": "2019-02-13T07:30:55.413654Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 9)                 81        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 90        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 9)                 90        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 40        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(9, input_shape=(8,)))\n",
    "model.add(Dense(9))\n",
    "model.add(Dense(9))\n",
    "model.add(Dense(4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "    \n",
    "TO DO: 위의 deep neural network 예에서 parameter 개수는 어떻게 계산이 되는가? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Anatomy of a neural network \n",
    "\n",
    "- Layers: network를 구성하는 요소 \n",
    "- Input and target: 학습과 예측을 위한 데이터\n",
    "- Loss function: 학습이 잘 되고 있는지 평가\n",
    "- Optimizer: 어떻게 학습을 진행하는가 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Layers\n",
    "- Tensor를 입력받아 tensor를 출력하는 데이터 처리 모듈 \n",
    "- 대부분 가중치를 가짐 (dropout, pooling과 같이 가중치가 없는 layer도 있음) \n",
    "- 많이 사용되는 Layer의 예\n",
    "    - Fully connected layer (dense layer)\n",
    "    - Convolution layer \n",
    "    - Recurrent layer \n",
    "    - Embedding layer \n",
    "    \n",
    "<font color='red'> Layers in Keras:\n",
    " https://keras.io/layers/about-keras-layers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Model\n",
    "- Layer를 쌓아 만드는 네트워크 \n",
    "- 일반적으로 하나의 input을 받아 하나의 output을 주는 층을 순서대로 쌓음 \n",
    "- 다양한 형태의 layer가 가능 (Keras Funtional API 활용: https://keras.io/getting-started/functional-api-guide/ )\n",
    "    - Input이 여러 개인 네트워크 \n",
    "    - Output이 여러 개인 네트워크 \n",
    "    - Inception block \n",
    "- 적절한 network architecture를 찾는 것은 과학 보다는 예술의 경지! 창의력이 필요함 \n",
    "- 기존의 잘 작동한 architecture를 모방하는 방식으로 접근 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Loss function\n",
    "- Model을 통해 나온 prediction $\\hat y^{(i)}$와 output $y^{(i)}$의 차이를 수량화 \n",
    "- 훈련하는 동안 최소화될 값 \n",
    "- 주어진 문제에 대한 성공 지표 \n",
    "\n",
    "<center> $J(w, b)= \\frac 1 m \\sum_{i=1}^m L(\\hat y^{(i)} ,y^{(i)})$ </center>\n",
    "\n",
    "- 해결하고자 하는 문제에 따라 표준적인 loss function이 존재함  \n",
    "    - Binary classification\n",
    "        - 두 개의 클래스를 분류\n",
    "        - 예) 문장을 입력하여 긍정/부정 구분\n",
    "        - Binary cross-entropy를 loss로 사용\n",
    "        $$ L(\\hat y^{(i)} ,y^{(i)}) = - y^{(i)} log(\\hat y^{(i)} ) - (1- y^{(i)}) log (1-\\hat y^{(i)} )$$\n",
    "\n",
    "    - Multi-class classification \n",
    "        - 두 개 이상의 클래스를 분류 \n",
    "        - 예) 이미지를 0,1,2,...,9로 구분\n",
    "        - Categorical cross-entropy를 loss로 사용 \n",
    "        $$ L(\\hat y^{(i)} ,y^{(i)}) = - \\sum_{c=1}^C y_c^{(i)} log(\\hat y_c^{(i)} ) $$\n",
    "\n",
    "    - Regression \n",
    "        - 연속형 값을 에측 \n",
    "        - 예) 주가 예측 \n",
    "        - Mean squared error를 loss로 사용 \n",
    "        $$L(\\hat y^{(i)} ,y^{(i)}) = \\frac  1 2 (\\hat y^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "\n",
    "<font color='red'> Loss functions in Keras: https://keras.io/losses/#available-loss-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cheatsheet_loss.PNG\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Optimizer\n",
    "- Loss function을 기반으로 네트워크가 어떻게 업데이트 될지를 결정하는 알고리즘  \n",
    "- 특정한 종류의 stochastic gradient descent 알고리즘을 구현 \n",
    "\n",
    "##### Gradient descent\n",
    "- Gradient: 특정한 점에서 함수 $f$의 기울기 \n",
    "- Gradient의 양의 방향 = 함수 $f$가 가장 빠르게 증가하는 방향 \n",
    "- Gradient의 음의 방향 = 함수 $f$가 가장 빠르게 감소하는 방향 \n",
    "\n",
    "    - 예) \n",
    "    $$ f(x)=x^2$$\n",
    "    $$ \\frac{df(x)}{dx} =2x$$\n",
    "        - $x=-1$에서의 gradient = $-2$\n",
    "        - $f(x)$가 가장 빠르게 감소하는 방향은 +2의 방향 \n",
    "    <img src=\"figures/gradient1.jpg\" width=\"350\">\n",
    "\n",
    "        <br><br>\n",
    "\n",
    "    - 예) \n",
    "    $$ f(x,y)=x^2+y^2 $$\n",
    "    $$ \\nabla f(x,y) = \\begin{bmatrix}\n",
    "    2x\\\\\n",
    "    2y \n",
    "    \\end{bmatrix}$$\n",
    "        - $(x,y)=(-1,-1)$에서의 gradient = $(-2, -2)^T$ \n",
    "        - $f(x)$가 가장 빠르게 감소하는 방향은 $(2, 2)^T$ \n",
    "    <img src=\"figures/gradient2.png\" width=\"350\">\n",
    "\n",
    "- Gradient descent algorithm \n",
    "$$ x^{(k+1)} = x^{(k)} - \\alpha \\nabla f(x^{(k)})$$ \n",
    "    <br>\n",
    "\n",
    "    -  초기값의 위치에서 가장 기울기가 가파른 방향($-\\nabla f$)으로  조금(<font color=red>$\\alpha$, learning rate, step size</font>) 이동\n",
    "    -  이동한 지점에서 다시 gradient를 계산하고 가장 기울기가 가파른 방향으로 조금 이동 \n",
    "    - Local minima로 수렴할 가능성이 있음 \n",
    "    - 초기값의 위치에 따라 수렴속도가 매우 느릴 수 있음 \n",
    "    ![](figures/fig6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic gradient descent (SGD)  algorithm\n",
    "- Gradient 계산 시 전체 데이터를 사용하지 않고 무작위로 선정된 일부 데이터(mini-batch)를 사용\n",
    "- 계산속도가 빠름\n",
    "- Local minima에 빠질 가능성을 줄여줌\n",
    "<img src=\"figures/fig7.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Others variants\n",
    "- weight를 업데이트 할 때 현재 ggradient 뿐만 아니라 이전에 업데이트된 weight, 기울기 등을 여러 방식으로 고려하려 SGD를 개선하는 방법들이 있음 \n",
    "- Adagrad, RMSProp, momentum, Adam 등이 많이 사용됨\n",
    "\n",
    "<img src=\"http://aikorea.org/cs231n/assets/nn3/opt2.gif\" width=\"40%\" align=\"left\"/>\n",
    "<img src=\"http://aikorea.org/cs231n/assets/nn3/opt1.gif\" width=\"40%\" align=\"center\"/>\n",
    "\n",
    "<cite> Images credit: [Alec Radford](https://twitter.com/alecrad) </cite>\n",
    "\n",
    "<font color='red'> Optimizers in Keras:  https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backpropagation\n",
    "- 신경망은 여러 개의 텐서 연산으로 연결되어 있음 \n",
    "$$ z = \\mathbb w^T \\mathbb x + b$$\n",
    "$$ y=\\sigma(z)$$\n",
    " $$L(\\mathbb w) = \\sum_i \\frac  1 2 (\\hat y^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "- $\\frac{dL}{dw_1}$을 어떻게 계산? \n",
    "- Chain rule을 이용 (연쇄법칙) \n",
    "    - 합성함수의 미분을 차례로 한 단계씩 계산해나가는 과정 \n",
    "    - $y=(x^2+1)^3, \\frac {dy} {dx}?$\n",
    "    \n",
    "    $g(x)=u=x^2+1$\n",
    "    \n",
    "    $y=f(u)=u^3$\n",
    "    \n",
    "    $\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} = (3u^2) (2x) = 3(x^2+1)^2 (2x)=6x(x^2+1)^2$\n",
    "    \n",
    "- Chain rule을 사용하여 loss function의 gradient를 계산 \n",
    "$$\\frac{dL}{dw_1} = \\frac{dL}{dy} \\frac{dy}{dz}\\frac{dz}{dw_1} $$\n",
    "    - Loss 값에 각 parameter가 기여한 정도를 계산하기 위해 최상층 부터 하위 층까지 거꾸로 연쇄법칙을 적용하여 계산 - backpropagation algorithm(reverse-mode automatic differentiation)  \n",
    "    - Tensorflow처럼 symbolic differentiation이 가능한 프레임워크를 사용하여 빠르게 계산 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T20:11:06.969865Z",
     "start_time": "2018-07-10T20:11:06.918278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not run\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Activation\n",
    "\n",
    "- Sigmoid (inverse-logit)\n",
    "\n",
    "    $$\\sigma(z) = \\frac 1 {1+e^{-z}}$$\n",
    "    - 0과 1사이의 값을 출력\n",
    "    - Output layer에서 확률값을 출력하고자 할 때 주로 사용 \n",
    "\n",
    "- Hypterbolic tangent\n",
    "    $$tanh(z) = 2σ(2z) – 1$$\n",
    "    - $-1<tanh(z)<1$\n",
    "    - Sigmoid의 이동한 형태\n",
    "    - Output이 0을 중심으로 분포하므로 sigmoid보다 학습에 효율적  \n",
    "    \n",
    "- ReLU(Rectified Linear Unit)\n",
    "    $$Relu(z)=max(0,z)$$\n",
    "    - 최적화 과정에서 gradient가 0과 가까워져 수렴이 느려지는 문제를 해결 \n",
    "    - 음수를 모두 0으로 처리하고 평균이 0이 되지 않는 다는 단점 \n",
    "    - Leaky ReLU, ELU 등 변화된 형태도 있음\n",
    "\n",
    "- Softmax\n",
    "    $$\\sigma(z_j) = \\frac{exp(z_j)}{\\sum_{k=1}^K exp(z_k)},  j=1,2, \\ldots, K$$\n",
    "    - 각 class의 score를 정규화 하여 각 class에 대한 확률값으로 변환(sum=1)\n",
    "    - Multi-class classification 문제의 output layer에서 사용 \n",
    "\n",
    "<font color='red'> Activation functions in Keras: https://keras.io/activations/\n",
    "    \n",
    "![](figures/fig5_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run\n",
    "model = Sequential()\n",
    "model.add(Dense(9, input_shape=(8,)))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "References\n",
    "- https://www.coursera.org/specializations/deep-learning\n",
    "- [Hands on Machine Learning with Scikit-Learn  and Tensorflow, Aurélien Géron]( http://www.hanbit.co.kr/store/books/look.php?p_code=B9267655530)\n",
    "- [Deep Learning with Python, François Chollet,](https://www.manning.com/books/deep-learning-with-python)\n",
    "- https://stanford.edu/~shervine/teaching/cs-229/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
