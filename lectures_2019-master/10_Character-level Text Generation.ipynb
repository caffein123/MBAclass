{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10. Character-level Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Text 처리 \n",
    "- 텍스트 데이터는 가장 흔한 시퀀스 형태의 데이터\n",
    "- Document classification, sentiment analysis, Question answering 등에 활용 \n",
    "- 텍스트 원본을 모형의 입력값으로 사용하지 못하기 때문에 텍스트를 수치형 텐서로 변환시키는 과정이 필요함\n",
    "\n",
    "- 텍스트 $\\rightarrow$ 토큰 $\\rightarrow$ 벡터 \n",
    "![](figures/token.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "        \n",
    "- 텍스트를 수치형 텐서로 변환하기 위해 나누는 단위\n",
    "    - 각 단어를 하나의 벡터로 변환\n",
    "            {“the”, “cat”, “sat”, “on”, “the”, “mat”, “.”}\n",
    "    - 각 문자를 하나의 벡터로 변환\n",
    "            {\"a\",\"c\",\"e\",\"h\",\"m\",\"n\",\"o\",\"s\",\"t\",\".\"}\n",
    "    - n-gram(연속된 단어나 문자의 그룹)을 추출하여 벡터로 변환 \n",
    "        - 2-grams\n",
    "                {\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n",
    "        - 3-grams\n",
    "                {\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\", \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    "\n",
    "- 토큰의 집합을 vocabulary, dictionary 라고 일컬음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어와 문자의 인코딩\n",
    "1. One-hot encoding\n",
    "    - 토큰을 벡터화 하는 가장 기본적인 방법\n",
    "    - 모든 단어에 고유한 정수 인덱스를 부여\n",
    "    - 정수 인덱스를 vocabulary size 크기의 binary 벡터로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "text = \" \".join(samples)\n",
    "\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "char2index = dict((c, i) for i, c in enumerate(sorted(chars)))\n",
    "index2char = dict((i, c) for i, c in enumerate(sorted(chars)))\n",
    "\n",
    "import numpy as np\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, len(char2index)))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        results[i, j, char2index[character]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Word embedding\n",
    "    - One-hot encoding을 하게 되면 한 단어를 나타내는 벡터의 길이가 vocabulary size와 같기 때문에 일반적으로 매우 고차원이고 대부분이 0으로 채워져 있음: 비효율적\n",
    "    - 0 또는 1로 채워진 고차원의 벡터 대신 실수값으로 채워져 있는 저차원 벡터로 표현하는 방법을 사용 \n",
    "    - 저차원에 더 많은 정보를 저장할 수 있어 효율적임\n",
    "    - Lecture 11에서 자세히 다룰 예정 \n",
    "\n",
    "![](figures/onehot_embed.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Character-level text generation model\n",
    "- 소설책의 텍스트를 학습하여 문장을 자동생성하는 모델\n",
    "-  Alice's Adventures in Wonderland\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      \n",
    "    ... Alice was beginning to get very tired of sitting by her sister\n",
    "    on the bank, and of having nothing to do:  once or twice she had\n",
    "    peeped into the book her sister was reading, but it had no\n",
    "    pictures or conversations in it, `and what is the use of a book,'\n",
    "    thought Alice `without pictures or conversation?' ...\n",
    "    \n",
    "- 첫 10개의 문자를 입력하여 다음에 나타날 문자를 예측\n",
    "\n",
    "<img src=\"figures/text_gen.png\" width=\"40%\" align=\"left\">\n",
    "<img src=\"figures/text_gen2.PNG\" width=\"30%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트를 한 줄씩 불러들여 소문자 변환, 빈 줄 삭제 등의 기본적인 전처리를 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T08:42:53.197378Z",
     "start_time": "2018-07-10T08:42:53.192152Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "INPUT_FILE = \"./data/alice_in_wonderland.txt\"\n",
    "\n",
    "fin = open(INPUT_FILE, 'rb') # 바이너리 파일을 읽기 모드로 오픈\n",
    "\n",
    "lines = []\n",
    "i=0\n",
    "for line in fin: # 파일을 한 줄씩 읽어들임\n",
    "    line = line.strip().lower() # 공백을 제거하고 소문자로 변환\n",
    "    line = line.decode(\"ascii\") # 디코딩하여 char로 변환\n",
    "    if len(line) == 0: # 빈 줄 삭제\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "\n",
    "text = \" \".join(lines) # 모든 줄을 하나로 이어붙임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문자 수준의 One-hot encoding을 하기 위해 유일한 문자들의 집합인 vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T08:42:53.223841Z",
     "start_time": "2018-07-10T08:42:53.212679Z"
    }
   },
   "outputs": [],
   "source": [
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* char와 index를 연결하는 lookup table 구축\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T08:42:53.223841Z",
     "start_time": "2018-07-10T08:42:53.212679Z"
    }
   },
   "outputs": [],
   "source": [
    "char2index = dict((c, i) for i, c in enumerate(sorted(chars)))\n",
    "index2char = dict((i, c) for i, c in enumerate(sorted(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2index['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2char[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input sequence와 output label 생성 \n",
    "- SEQLEN: 다음 문자를 예측하기 위해 입력할 문자의 수 \n",
    "- STEP: 몇 개씩 건너뛰며 window를 이동할 것인가? \n",
    "- Ex: Input text= \"The sky was falling\"\n",
    "    - The sky wa -> s\n",
    "    - he sky was -> \" \"  \n",
    "    - e sky was  -> f\n",
    "    - sky was f -> a\n",
    "    - sky was fa -> l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T08:42:53.318734Z",
     "start_time": "2018-07-10T08:42:53.225761Z"
    }
   },
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "\n",
    "input_chars = []\n",
    "label_chars = []\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T08:42:53.326050Z",
     "start_time": "2018-07-10T08:42:53.320449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"alice's ad\",\n",
       "  \"lice's adv\",\n",
       "  \"ice's adve\",\n",
       "  \"ce's adven\",\n",
       "  \"e's advent\",\n",
       "  \"'s adventu\",\n",
       "  's adventur',\n",
       "  ' adventure',\n",
       "  'adventures',\n",
       "  'dventures '],\n",
       " ['v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_chars[0:10],label_chars[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143504"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 만든 input/output 문자 셋을 one-hot encoding으로 변환하여 모형에 입력 가능한 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T08:42:53.945087Z",
     "start_time": "2018-07-10T08:42:53.328270Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input과 output chars를 nb_chars 길이의 one-hot vector로 표현 \n",
    "- input\n",
    "    - (len(input_chars), SEQLEN, nb_chars)\n",
    "    - SEQLEN 개 시점의 nb_chars 차원의 벡터가 input shape\n",
    "- output\n",
    "    - (len(input_chars), nb_chars)\n",
    "    - (SEQLEN, nb_chars) 차원의 각 input에 대응하는 output label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T08:42:53.952917Z",
     "start_time": "2018-07-10T08:42:53.947986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice's ad\n",
      "(10, 45)\n",
      "(143504, 45)\n"
     ]
    }
   ],
   "source": [
    "print(input_chars[0])\n",
    "print(X[0].shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T08:42:54.273176Z",
     "start_time": "2018-07-10T08:42:53.955909Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114803 samples, validate on 28701 samples\n",
      "Epoch 1/100\n",
      "114803/114803 [==============================] - 25s 220us/step - loss: 2.9172 - val_loss: 2.6408\n",
      "Epoch 2/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 2.3817 - val_loss: 2.2453\n",
      "Epoch 3/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 2.1555 - val_loss: 2.1247\n",
      "Epoch 4/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 2.0612 - val_loss: 2.0505\n",
      "Epoch 5/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 1.9960 - val_loss: 2.0086\n",
      "Epoch 6/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 1.9501 - val_loss: 1.9572\n",
      "Epoch 7/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 1.9082 - val_loss: 1.9337\n",
      "Epoch 8/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 1.8765 - val_loss: 1.8940\n",
      "Epoch 9/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 1.8502 - val_loss: 1.8785\n",
      "Epoch 10/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 1.8246 - val_loss: 1.8592\n",
      "Epoch 11/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 1.8039 - val_loss: 1.8434\n",
      "Epoch 12/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 1.7850 - val_loss: 1.8287\n",
      "Epoch 13/100\n",
      "114803/114803 [==============================] - 24s 209us/step - loss: 1.7646 - val_loss: 1.8291\n",
      "Epoch 14/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 1.7482 - val_loss: 1.8150\n",
      "Epoch 15/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 1.7374 - val_loss: 1.7821\n",
      "Epoch 16/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 1.7205 - val_loss: 1.7854\n",
      "Epoch 17/100\n",
      "114803/114803 [==============================] - 24s 209us/step - loss: 1.7082 - val_loss: 1.7820\n",
      "Epoch 18/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 1.6943 - val_loss: 1.7651\n",
      "Epoch 19/100\n",
      "114803/114803 [==============================] - 24s 210us/step - loss: 1.6844 - val_loss: 1.7390\n",
      "Epoch 20/100\n",
      "114803/114803 [==============================] - 24s 212us/step - loss: 1.6722 - val_loss: 1.7446\n",
      "Epoch 21/100\n",
      "114803/114803 [==============================] - 25s 220us/step - loss: 1.6643 - val_loss: 1.7307\n",
      "Epoch 22/100\n",
      "114803/114803 [==============================] - 26s 224us/step - loss: 1.6523 - val_loss: 1.7386\n",
      "Epoch 23/100\n",
      "114803/114803 [==============================] - 25s 218us/step - loss: 1.6428 - val_loss: 1.7215\n",
      "Epoch 24/100\n",
      "114803/114803 [==============================] - 24s 212us/step - loss: 1.6342 - val_loss: 1.7191\n",
      "Epoch 25/100\n",
      "114803/114803 [==============================] - 25s 217us/step - loss: 1.6276 - val_loss: 1.7218\n",
      "Epoch 26/100\n",
      "114803/114803 [==============================] - 25s 220us/step - loss: 1.6183 - val_loss: 1.7120\n",
      "Epoch 27/100\n",
      "114803/114803 [==============================] - 25s 219us/step - loss: 1.6096 - val_loss: 1.7115\n",
      "Epoch 28/100\n",
      "114803/114803 [==============================] - 25s 221us/step - loss: 1.6026 - val_loss: 1.7029\n",
      "Epoch 29/100\n",
      "114803/114803 [==============================] - 25s 220us/step - loss: 1.5946 - val_loss: 1.6881\n",
      "Epoch 30/100\n",
      "114803/114803 [==============================] - 25s 219us/step - loss: 1.5854 - val_loss: 1.6793\n",
      "Epoch 31/100\n",
      "114803/114803 [==============================] - 25s 218us/step - loss: 1.5785 - val_loss: 1.6871\n",
      "Epoch 32/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 1.5723 - val_loss: 1.6834\n",
      "Epoch 33/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 1.5677 - val_loss: 1.6772\n",
      "Epoch 34/100\n",
      "114803/114803 [==============================] - 24s 211us/step - loss: 1.5602 - val_loss: 1.6840\n",
      "Epoch 35/100\n",
      "114803/114803 [==============================] - 25s 214us/step - loss: 1.5542 - val_loss: 1.6791\n",
      "Epoch 36/100\n",
      "114803/114803 [==============================] - 25s 219us/step - loss: 1.5489 - val_loss: 1.6718\n",
      "Epoch 37/100\n",
      "114803/114803 [==============================] - 25s 220us/step - loss: 1.5437 - val_loss: 1.6765\n",
      "Epoch 38/100\n",
      "114803/114803 [==============================] - 25s 221us/step - loss: 1.5379 - val_loss: 1.6740\n",
      "Epoch 39/100\n",
      "114803/114803 [==============================] - 27s 239us/step - loss: 1.5339 - val_loss: 1.6658\n",
      "Epoch 40/100\n",
      "114803/114803 [==============================] - 25s 219us/step - loss: 1.5283 - val_loss: 1.6546\n",
      "Epoch 41/100\n",
      "114803/114803 [==============================] - 26s 226us/step - loss: 1.5226 - val_loss: 1.6558\n",
      "Epoch 42/100\n",
      "114803/114803 [==============================] - 28s 241us/step - loss: 1.5175 - val_loss: 1.6513\n",
      "Epoch 43/100\n",
      "114803/114803 [==============================] - 26s 225us/step - loss: 1.5141 - val_loss: 1.6681\n",
      "Epoch 44/100\n",
      "114803/114803 [==============================] - 25s 219us/step - loss: 1.5090 - val_loss: 1.6435\n",
      "Epoch 45/100\n",
      "114803/114803 [==============================] - 25s 219us/step - loss: 1.5042 - val_loss: 1.6458\n",
      "Epoch 46/100\n",
      "114803/114803 [==============================] - 26s 224us/step - loss: 1.5014 - val_loss: 1.6643\n",
      "Epoch 47/100\n",
      "114803/114803 [==============================] - 26s 223us/step - loss: 1.4971 - val_loss: 1.6466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9059ecaf60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard \n",
    "import time \n",
    "\n",
    "HIDDEN_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=True, input_shape=(SEQLEN, nb_chars), activation='relu'))\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(HIDDEN_SIZE, activation='relu'))\n",
    "model.add(Dense(nb_chars, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.adam(lr=0.001))\n",
    "\n",
    "now = time.strftime(\"%c\")\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(filepath='models/text_gen.h5', monitor='val_loss', save_best_only=True),\n",
    "    TensorBoard(log_dir='logs/text_generation/'+now),\n",
    "    EarlyStopping(monitor='val_loss',patience=3)\n",
    "]\n",
    "model.fit(X, y, batch_size=BATCH_SIZE, epochs=100, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prediction\n",
    "\n",
    "- random seed 선택 (10개의 char로 이루어진 부분 문장)\n",
    "- 다음번 문자로 나타날 확률이 가장 높은 문자 프린트 (ypred)\n",
    "- \"앞에서 사용된 9개 문자 + 새로 발생된 문자 1개\"를 input으로 사용 \n",
    "- 반복을 통해 지정한 개수 만큼 문자 발생 \n",
    "<img src=\"figures/text_gen_pred.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문자열을 입력하여 그 다음에 나타날 확률일 가장 높은 문자를 무조건 발생시키면 비슷한 상황에서 언제나 같은 문자를 만들어냄\n",
    "- 무조건 가장 높은 문자를 출력하는 대신 확률적으로 발생시킨다면 보다 다양한 문장을 생성할 수 있음 \n",
    "    - e.g) {\"a\",\"b\",\"c\"}에 대한 output이 {0.2, 0.5, 0.3}인 경우 언제나 \"b\"를 출력하기 보다는 0.5의 확률로 \"b\"를 출력(multinomial distribution 활용)\n",
    "- Output 값이 가장 큰 문자를 얼마나 \"확실히\" 출력할 것인가? temperature 값으로 조정하기 위해 `sample` 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    #print(preds.round(3))\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample([0.2, 0.5, 0.3], temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model('models/text_gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T09:12:39.962596Z",
     "start_time": "2018-07-10T09:12:38.857949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from seed:  as usual.\n",
      " as usual.  `i dinmore the dacke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s the poor the harn, `and she parting the little work it your, and she was a better the mouted to the more pats to ofs were had fring in in the doran a lonfed ait the ding to on the in a little.  `i was she had to the paterny and asking the mouse of the queen sught the hersels and the cat, and chear bemand and she in thing the more said she began in cleng to the drears the hu"
     ]
    }
   ],
   "source": [
    "test_idx = np.random.randint(len(input_chars))\n",
    "test_chars = input_chars[test_idx]\n",
    "print(\"Generating from seed: %s\" % (test_chars))\n",
    "print(test_chars, end=\"\")\n",
    "for i in range(400):\n",
    "    Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "    for i, ch in enumerate(test_chars):\n",
    "        Xtest[0, i, char2index[ch]] = 1\n",
    "    pred = model.predict(Xtest, verbose=0)[0]\n",
    "    ypred = index2char[sample(pred, 0.5)]\n",
    "    print(ypred, end=\"\")\n",
    "    # move forward with test_chars + ypred\n",
    "    test_chars = test_chars[1:] + ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
