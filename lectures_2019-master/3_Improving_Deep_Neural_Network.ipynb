{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color=\"darkgreen\">\n",
    "\n",
    "# Lecture 3.  Improving Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Overfitting and underfitting\n",
    "- Optimization vs. generalization\n",
    "    - Optimization: train data에서 최고의 성능을 얻으려고 모델을 조정하는 과정 \n",
    "    - Generalization: 훈련된 모델이 처음 보는 데이터에서 얼마나 잘 수행되는지를 의미\n",
    "\n",
    "- Underfitting의 발생 \n",
    "    - epoch가 진행될 수록 train loss와 test loss(validation loss)가 함께 낮아짐 \n",
    "    - 모델의 성능이 발전될 여지가 있음 (optimization을 더 할 여지가 있음) \n",
    "\n",
    "- Overfitting의 발생 \n",
    "    - epoch가 진행되면서 train loss는 계속 감소하지만 test loss(validation loss)가 증가하기 시작함\n",
    "    - 훈련 데이터에 특화된 패턴을 학습하기 시작하여 새로운 데이터에 대해 잘못된 판단을 함 \n",
    "  \n",
    "<img src=\"figures/bias.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"figures/overfitting.png\" width=\"80%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting을 방지하기 위한 방법\n",
    "    - 더 많은 train data를 수집\n",
    "    - 모델의 복잡성을 축소 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Reducing the network size\n",
    "- 모델의 크기를 줄여 학습 파라메터 수를 줄임\n",
    "- Layer의 수, 각 layer의 unit 수를 조정 \n",
    "- 적은 수의 layer, unit에서 시작해서 증가시켜 가면서 validation loss의 감소 추세를 관찰 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:39:19.453284Z",
     "start_time": "2019-02-19T01:39:11.764482Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:40:36.133865Z",
     "start_time": "2019-02-19T01:40:36.035333Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "original_model = models.Sequential()\n",
    "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "original_model.add(layers.Dense(16, activation='relu'))\n",
    "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "original_model.compile(optimizer='rmsprop',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:40:36.787558Z",
     "start_time": "2019-02-19T01:40:36.508381Z"
    }
   },
   "outputs": [],
   "source": [
    "smaller_model = models.Sequential()\n",
    "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "smaller_model.add(layers.Dense(4, activation='relu'))\n",
    "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "smaller_model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:41:24.089791Z",
     "start_time": "2019-02-19T01:40:37.278856Z"
    }
   },
   "outputs": [],
   "source": [
    "original_hist = original_model.fit(x_train, y_train,\n",
    "                                   epochs=20,\n",
    "                                   batch_size=256,\n",
    "                                   validation_data=(x_test, y_test),\n",
    "                                   verbose=2)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.save('original_model.h5')  ## 모델 저장 \n",
    "\n",
    "## model = models.load_model('original_model.h5') ## 저장된 모델 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:42:11.255159Z",
     "start_time": "2019-02-19T01:41:24.706351Z"
    }
   },
   "outputs": [],
   "source": [
    "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
    "                                       epochs=20,\n",
    "                                       batch_size=256,\n",
    "                                       validation_data=(x_test, y_test),\n",
    "                                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:42:20.419782Z",
     "start_time": "2019-02-19T01:42:20.413386Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "original_val_loss = original_hist.history['val_loss']\n",
    "smaller_model_val_loss = smaller_model_hist.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T01:42:30.383009Z",
     "start_time": "2019-02-19T01:42:30.190471Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# b+ is for \"blue cross\"\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T02:05:00.319239Z",
     "start_time": "2019-02-19T02:05:00.114460Z"
    }
   },
   "outputs": [],
   "source": [
    "original_train_loss = original_hist.history['loss']\n",
    "smaller_model_train_loss = smaller_model_hist.history['loss']\n",
    "\n",
    "plt.plot(epochs, original_train_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, smaller_model_train_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "TO DO: Original model보다 더 큰 네트워크를 구성하고 모형을 적합시킨 후 validation loss와 train loss를 original model과 비교하시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Smaller model\n",
    "    - training loss 감소 속도는 더 느림 \n",
    "    - overfitting이 발생할 여지가 적음 \n",
    "    - 충분히 train loss가 감소하지 않을 수 있음 (즉, optimization이 덜 됨)\n",
    "    - validation loss가 최소화 될 때까지 걸리는 시간이 오래 걸림 \n",
    "- Bigger model\n",
    "    - validation loss가 초기부터 증가 추세: overfitting 발생 \n",
    "    - Bigger model의 train loss는 빠르게 감소 \n",
    "    - train data에 대해 optimization이 잘 되지만 validation set에 대해 성능이 좋지 않음 \n",
    "    \n",
    "- 어떻게 최적의 네트워크를 찾는가? \n",
    "    - 작은 크기의 모형에서부터 시작 \n",
    "    - 네트워크 크기를 증가시키면서 (layer의 수, 각 layer의 unit의 수 증가) train loss가 충분히 감소하는지, validation loss가 증가하는지 체크 \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Adding weight regularization \n",
    "- weight가 작은 값을 갖도록 강제하여 weight 값의 분포가 더 균일하게 되도록 만듬 \n",
    "- 큰 weight에 대한 penalty를 loss function에 추가\n",
    "    - L1 penalty: weight의 l1-norm $\\sum |w_j |$에 비례하는 penalty를 추가 \n",
    "    - L2 penalty: weight의 l2-norm $\\sum w_j^2$에 비례하는 penalty를 추가 \n",
    "- Keras의 layer에서 `kernel_regularizer` option으로 penalty 추가 \n",
    "    - L1 penalty: `regularizers.l1(0.001)`\n",
    "    - L2 penalty: `regularizers.l2(0.001)`\n",
    "    - L1&L2 penalty: `regularizers.l1_l2(l1=0.001, l2=0.001)`\n",
    "- penalty term은 training 과정에서만 loss function에 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T02:38:10.707841Z",
     "start_time": "2019-02-19T02:38:10.609445Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "K.clear_session()\n",
    "\n",
    "l2_model = models.Sequential()\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu', input_shape=(10000,)))\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu'))\n",
    "l2_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "l2_model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T02:38:58.420716Z",
     "start_time": "2019-02-19T02:38:11.149266Z"
    }
   },
   "outputs": [],
   "source": [
    "l2_model_hist = l2_model.fit(x_train, y_train,\n",
    "                             epochs=20,\n",
    "                             batch_size=256,\n",
    "                             validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T02:40:33.518401Z",
     "start_time": "2019-02-19T02:40:33.318314Z"
    }
   },
   "outputs": [],
   "source": [
    "l2_model_val_loss = l2_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 penalty를 추가하였을 때 original model에 비해 validation loss가 천천히 증가함. 즉, overfitting이 덜 되도록 학습되고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Dropout regularization\n",
    "- Neural network를 위해 사용되는 regularization 기법 중에서 가장 효과적이고 널리 사용되는 방법 중 하나\n",
    "- Random하게 선택된 node를 매 iteration에서 제외하고 training 진행 \n",
    "- 일반적으로 dropout rate=0.2~0.5\n",
    "- Test set에 대해서는 적용하지 않음 \n",
    "- Keras에서 `Dropout` layer를 추가 \n",
    "<img src=\"figures/dropout.PNG\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T03:17:29.131949Z",
     "start_time": "2019-02-19T03:17:29.016880Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "dpt_model = models.Sequential()\n",
    "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(16, activation='relu'))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "dpt_model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T03:18:21.424687Z",
     "start_time": "2019-02-19T03:17:32.405054Z"
    }
   },
   "outputs": [],
   "source": [
    "dpt_model_hist = dpt_model.fit(x_train, y_train,\n",
    "                               epochs=20,\n",
    "                               batch_size=256,\n",
    "                               validation_data=(x_test, y_test),\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T03:18:42.872289Z",
     "start_time": "2019-02-19T03:18:42.678262Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dpt_model_val_loss = dpt_model_hist.history['val_loss']\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "plt.plot(epochs, dpt_model_val_loss, 'bo', label='Dropout-regularized model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Monitoring deep learning models using Keras callbacks and  Tensorboard\n",
    "#### 3.2.1 Keras Callbacks\n",
    "\n",
    "- overfitting을 막기 위해서는 모형을 학습하는 중간에  epoch 수에 따라 변하는 여러 measure들을 확인할 필요가 있음 \n",
    "- `EarlyStopping`: Validation set에 대한 measure가  더 이상 개선되지 않을 때 학습을 자동으로 멈춤\n",
    "    - `monitor='acc'`: validation accuracy를 기준으로  학습 중단여부 판단 \n",
    "    - `patience=1`: validation accuracy가 개선이 안되더라도 1 epoch는 기다린 후 여전\n",
    "- `ModelCheckpoint`: 지정한 measure(예:validation loss)가 최소값일 때의 모델과 weight를 저장하여 overfitting이 발생하기 전의 model을 나중에 불러들여 사용할 수 있음\n",
    "    - `save_best_only=True`: monitoring 중인 measure를 기준으로 최적의 모형의 weight만 저장 \n",
    "\n",
    "\n",
    "https://keras.io/callbacks/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Tensorboard\n",
    "- 모형이 학습되는 과정 중에 metrics를 monitoring하여 시각화 \n",
    "    - 사용자가 지정한 metrics(`model.compile`의 `metrics` 옵션에서 지정)를 시각적으로 monitor\n",
    "    - 모형의 구조를 시각화\n",
    "    - activation, gradient의 분포를 시각화\n",
    "    <img src=\"figures/tensorboard.PNG\" width=\"50%\">\n",
    "\n",
    "- Keras를 Tensorflow backend로 사용할 때만 지원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:27:55.862455Z",
     "start_time": "2019-02-19T04:27:55.809657Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tensorboard_model = models.Sequential()\n",
    "tensorboard_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "tensorboard_model.add(layers.Dense(16, activation='relu'))\n",
    "tensorboard_model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:27:56.416164Z",
     "start_time": "2019-02-19T04:27:56.408505Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import time\n",
    "\n",
    "now = time.strftime(\"%c\")\n",
    "\n",
    "callbacks_list = [\n",
    "    EarlyStopping(monitor='val_loss',patience=1),\n",
    "    ModelCheckpoint(filepath='movie_review'+now+'.h5',monitor='val_loss',save_best_only=True),\n",
    "    TensorBoard(log_dir='./logs/movie_review'+now, histogram_freq=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:28:42.514737Z",
     "start_time": "2019-02-19T04:27:57.054182Z"
    }
   },
   "outputs": [],
   "source": [
    "tensorboard_model.compile(optimizer='rmsprop',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])\n",
    "tensorboard_model.fit(x_train, y_train,\n",
    "                       epochs=20,\n",
    "                       batch_size=256,\n",
    "                       validation_data=(x_test, y_test),\n",
    "                       verbose=2, \n",
    "                       callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `log_dir`: training log를 저장할 directory 지정 \n",
    "- `model.fit` 실행 후 터미널 환경에서 `tensorboard --logdir=(log_dir에서 지정한 위치)` 명령어 실행\n",
    "- `http://localhost:6006`로 접속하여 tensorboard 확인 (수업의 서버 환경에서는 세션 생성 화면에서 버튼을 눌러 실행)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/tensorboard2.PNG\" width=\"70%\">\n",
    "<img src=\"figures/tensorboard3.PNG\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Hyperparameter tuning\n",
    "\n",
    "\n",
    "- Parameters\n",
    "    - 주어진 모델에 대한 loss 함수의 변수들\n",
    "        - Weights $W$\n",
    "        - Bias $b$\n",
    "- Hypterparameters\n",
    "    - 모형의 구조를 결정하거나 optimization 방법을 결정하는 변수들 \n",
    "    - $W$, $b$를 최종적으로 결정 \n",
    "        - Optimizer의 종류\n",
    "        - learning rate($\\alpha$)\n",
    "        - Hidden layer의 수 \n",
    "        - Hidden unit의 수 \n",
    "        - Iteration의 수 \n",
    "        - Activation function의 종류\n",
    "        - Minibatch size \n",
    "        - Regularization\n",
    "    \n",
    "- Applied deep learning is a very empirical process.\n",
    "- 다양한 조합의 hyperparameter를 시도해서 loss 함수가 빠르게 감소하는 hypterparameter를 찾아내는 시도가 필요 \n",
    "\n",
    "<img src=\"figures/emp.png\" width=\"30%\" align=\"left\">\n",
    "<img src=\"figures/lr.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Hyperparameter가 모형 성능을 좌우 \n",
    "- Systematic hyperparameter search가 중요 \n",
    "- 고려해야 할 주요한 hyperparameters\n",
    "    - Learning rate($\\alpha$): 일반적으로 가장 중요 \n",
    "    - Hidden units의 수 \n",
    "    - Mini-batch size \n",
    "    - Momentum 값, Adam의 $\\beta$ 값 \n",
    "    - Layers의 수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Grid search 보다는 random search가 효율적\n",
    "    - 중요한 parameter에 대해 더 많은 값에 대해 탐색 가능 \n",
    "    <img src=\"figures/gridsearch.png\" width=\"80%\">\n",
    "\n",
    "- Coarse to fine search \n",
    "    - 듬성듬성한 random search 후 성능이 좋은 값 주변을 보다 조밀하게 탐색\n",
    "    <img src=\"figures/coarse-to-fine.png\" width=\"40%\">\n",
    "\n",
    "- Appropriate scale for hyperparameter search\n",
    "    - Hidden unit의 수, layer의 수 \n",
    "        - 예) 50~100 범위에서 균일한 random number의 hidden unit 수 고려 \n",
    "    - Learning rate $\\alpha$ \n",
    "        - Log scale에서 random number 고려 \n",
    "        - 작은 $\\alpha$ 부분을 보다 촘촘하게 sampling\n",
    "            <img src=\"figures/logscale.PNG\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:36:24.301607Z",
     "start_time": "2019-02-19T04:36:24.296555Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r=-4*np.random.rand(10)\n",
    "alpha=10**r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:36:25.115702Z",
     "start_time": "2019-02-19T04:36:25.110382Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:36:35.836441Z",
     "start_time": "2019-02-19T04:36:28.188825Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "now = time.strftime(\"%c\")\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(filepath='movie_review'+now+'.h5',monitor='val_loss',save_best_only=True),\n",
    "    TensorBoard(log_dir='./logs/movie_review'+now, histogram_freq=1)\n",
    "]\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.rmsprop(lr=alpha[0]),\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "               epochs=20,\n",
    "               batch_size=256,\n",
    "               validation_data=(x_test, y_test),\n",
    "               verbose=2, \n",
    "               callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T04:36:44.141423Z",
     "start_time": "2019-02-19T04:36:44.134632Z"
    }
   },
   "outputs": [],
   "source": [
    "np.min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 The universal workflow of machine learning \n",
    "\n",
    "#### (1) 문제 정의와 데이터셋 수집 \n",
    "- 무엇을 예측하려 하는가? 이를 예측하기 위한 training data가 있는가? \n",
    "    - Ex) 영화 리뷰의 감성 분류를 학습하기 위해서는 각 영화 리뷰의 감성 레이블이 태깅되어 있는 데이터가 있어야 함 \n",
    "\n",
    "- 예측하려는 문제의 종류는 무엇인가?\n",
    "    - Binary classification\n",
    "    - Multi-class classification \n",
    "    - Multi-label classification\n",
    "    - Regression\n",
    "    - Clustering\n",
    "    - Reinforcement learning \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 성공 지표의 선택\n",
    "- ROC AUC는 일반적인 지표 \n",
    "- 클래스 분포가 균일하지 않다면 precision(=TP/(TP+FP)), recall(=TP/(TP+FN))을 사용할 수 있음\n",
    "- multi-label classification인 경우 average precision 사용 \n",
    "\n",
    "#### (3) 평가 방법 선택\n",
    "- Hold-out validation set 사용\n",
    "    - Train data의 일정 부분을 validation set으로 사용 \n",
    "    - 데이터의 양이 많을 때 사용하는 방법\n",
    "    - `keras.model.fit`의 `validation_data` 또는 `validation_split` option\n",
    "- K-fold cross-validation\n",
    "    - Train set을 K-개의 무작위 set으로 구분한 뒤 하나씩 validation set으로 사용하며 반복\n",
    "    - Hold-out validation set을 구성하기에 데이터가 적을 때 사용  \n",
    "    - Keras 자체의 cv 모듈이 없으므로 scikit-learn의 `KFold`를 사용 (참고: https://3months.tistory.com/321)\n",
    "- Iterated K-fold cross-validation \n",
    "    - K-fold CV를 반복\n",
    "    - 데이터가 적으나 정확한 모델 평가가 필요할 때 사용 \n",
    "\n",
    "#### (4) 데이터 준비 \n",
    "- Input data는 일반적으로 [-1,1] 혹은 [0,1] 사이의 데이터로 스케일 조정\n",
    "- 사용하려는 모델에 맞는 input 형태로 조정 \n",
    "\n",
    "#### (5) Baseline보다 나은 모델 훈련 \n",
    "- 이진분류 라면 정확도 0.5, MNIST 데이터라면 정확도 0.1보다 높은 모형 만들기 \n",
    "- 마지막 layer의 activation function 선택\n",
    "    - output의 형태에 따라 조정\n",
    "    - Sigmoid, softmax, linear 등\n",
    "- Loss function 선택\n",
    "    - 풀고자 하는 문제의 종류에 따라 선택\n",
    "    - binary_crossentropy, categorical_crossentropy, mse 등\n",
    "    - 미분 가능해야 하고 주어진 mini-batch에서 계산 가능해야 함. (ROC AUC 등은 사용 불가) \n",
    "- Optimizer와 learning rate 선택 \n",
    "    - rmsprop, adam과 default learning rate 사용이 무난 \n",
    "    \n",
    "<img src=\"figures/cheatsheet_loss.PNG\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Scaling up: overfitting 모델 구축 \n",
    "- 충분한 성능을 나타내는 모델을 만들기 위해 모형을 크게 확장 \n",
    "    - Layer 추가 \n",
    "    - 각 layer의 unit 추가 \n",
    "    - Epoch 수 증가 \n",
    "- train loss와 validation loss를 모니터 \n",
    "\n",
    "#### (7) Regularization, hyperparameter tuning \n",
    "- 반복적으로 모델 수정, 훈련, 평가하며 모델 튜닝 \n",
    "    - Dropout 추가 \n",
    "    - Layer 추가 혹은 제거 \n",
    "    - L1 또는 L2 penalty 추가 \n",
    "    - Layer의 unit 수나 learning rate의 튜닝 \n",
    "    \n",
    "- 주의: 이 과정을 반복한다는 것은 validation set에  대해 모델이 적합되고 있는 것이기 때문에 이후 test set에서의 성능이 validation 성능보다 낮아질 수 있음 (즉, validation set에 overfitting) \n",
    "    - K-fold CV/ iterated K-fold CV 등으로 검증 방법 바꾸는 것도 방법 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "        \n",
    "References\n",
    "- https://www.coursera.org/specializations/deep-learning\n",
    "- [Hands on Machine Learning with Scikit-Learn  and Tensorflow, Aurélien Géron]( http://www.hanbit.co.kr/store/books/look.php?p_code=B9267655530)\n",
    "- [Deep Learning with Python, François Chollet,](https://www.manning.com/books/deep-learning-with-python)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
