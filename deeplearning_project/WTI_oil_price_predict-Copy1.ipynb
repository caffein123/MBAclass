{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install requests\n",
    "!pip install --upgrade pandas\n",
    "!pip install konlpy\n",
    "!pip install gensim\n",
    "!cp -R /home/work/deep/jdk1.8.0_201 /usr/lib/jvm\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('kyoh', 'xLxYyOTECJ48ofwzrn2j')\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1) 국제 원유가격 데이터에서 예측할 두바이유 데이터 로드 및 추세 확인\n",
    "### 2) 피쳐로 사용할 데이터 탐색 \n",
    "### - 국제 유가 관련 뉴스 데이터\n",
    "### - 다우 존스 데이터\n",
    "### - 환율 데이터\n",
    "### - 코스피 데이터\n",
    "### - Brent 데이터\n",
    "### - WTI 데이터\n",
    "### - 국제 유가에 주가 상승 영향을 받는 국내 주식 종목\n",
    "###   (SK이노베이션,S-OIL,금호석유)\n",
    "### - 국제 유가에 주가 하락 영향을 받는 국내 주식 종목\n",
    "###   (아시아나항공,대한항공,현대상선)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'time':timestamp,'SK이노베이션':data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 국제 원유가격(Brent, WTI, 두바이) 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_price = pd.read_csv('국제_원유가격20080607_20190610.csv', encoding='UTF-8')\n",
    "df_oil_price.columns = ['time','dubai','brent','WIT']\n",
    "\n",
    "df_oil_price['time'] = '20'+df_oil_price.time\n",
    "df_oil_price['time'] = pd.to_datetime(df_oil_price['time'], format='%Y년%m월%d일')\n",
    "#df_oil_price = df_oil_price.drop(df.index[list(df_WTI[df_WTI.price=='.'].index)])\n",
    "#df_WTI['price']= df_WTI.price.astype('float')\n",
    "#df=df.merge(df_oil,how='outer',on=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 중 이상치가 있으면 전일 값, 전전일 값으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_price.dubai = np.where(df_oil_price.dubai == '-', df_oil_price.dubai.shift(-1) , df_oil_price.dubai)\n",
    "df_oil_price.dubai = np.where(df_oil_price.dubai == '-', df_oil_price.dubai.shift(-2) , df_oil_price.dubai)\n",
    "df_oil_price.brent = np.where(df_oil_price.brent == '-', df_oil_price.brent.shift(-1) , df_oil_price.brent)\n",
    "df_oil_price.brent = np.where(df_oil_price.brent == '-', df_oil_price.brent.shift(-2) , df_oil_price.brent)\n",
    "df_oil_price.WIT = np.where(df_oil_price.WIT == '-', df_oil_price.WIT.shift(-1) , df_oil_price.WIT)\n",
    "df_oil_price.WIT = np.where(df_oil_price.WIT == '-', df_oil_price.WIT.shift(-2) , df_oil_price.WIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_price.dubai = df_oil_price.dubai.astype(float)\n",
    "df_oil_price.brent = df_oil_price.brent.astype(float)\n",
    "df_oil_price.WIT = df_oil_price.WIT.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dubai</th>\n",
       "      <th>brent</th>\n",
       "      <th>WIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2840.000000</td>\n",
       "      <td>2840.000000</td>\n",
       "      <td>2840.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>77.314197</td>\n",
       "      <td>80.033173</td>\n",
       "      <td>73.468486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.929948</td>\n",
       "      <td>26.372873</td>\n",
       "      <td>23.115252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>22.830000</td>\n",
       "      <td>27.880000</td>\n",
       "      <td>26.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54.167500</td>\n",
       "      <td>56.007500</td>\n",
       "      <td>51.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>73.750000</td>\n",
       "      <td>75.585000</td>\n",
       "      <td>72.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.472500</td>\n",
       "      <td>107.737500</td>\n",
       "      <td>93.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>140.700000</td>\n",
       "      <td>146.080000</td>\n",
       "      <td>145.290000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dubai        brent          WIT\n",
       "count  2840.000000  2840.000000  2840.000000\n",
       "mean     77.314197    80.033173    73.468486\n",
       "std      25.929948    26.372873    23.115252\n",
       "min      22.830000    27.880000    26.210000\n",
       "25%      54.167500    56.007500    51.697500\n",
       "50%      73.750000    75.585000    72.480000\n",
       "75%     104.472500   107.737500    93.825000\n",
       "max     140.700000   146.080000   145.290000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#기술 통계\n",
    "df_oil_price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kyoh/25.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_brent = go.Scatter(\n",
    "                x=df_oil_price.time,\n",
    "                y=df_oil_price['brent'],\n",
    "                name = \"brent\",\n",
    "                line = dict(color = '#89d5c9'),\n",
    "                opacity = 0.8)\n",
    "\n",
    "trace_dubai = go.Scatter(\n",
    "                x=df_oil_price.time,\n",
    "                y=df_oil_price['dubai'],\n",
    "                name = \"dubai\",\n",
    "                line = dict(color = '#e25b45'),\n",
    "                opacity = 0.8)\n",
    "\n",
    "trace_WIT = go.Scatter(\n",
    "                x=df_oil_price.time,\n",
    "                y=df_oil_price['WIT'],\n",
    "                name = \"WIT\",\n",
    "                line = dict(color = '#adc965'),\n",
    "                opacity = 0.8)\n",
    "data = [trace_brent,trace_dubai,trace_WIT]\n",
    "layout = dict(\n",
    "    title = \"국제 유가 추이\",)\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename = \"국제 유가 추이\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Data Load(국제유가 관련 뉴스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/NewsMainOutput.csv', encoding='UTF-8',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main']= df.main.str.replace(\"\\t\",\"\")\n",
    "df['main']= df['main'].str.replace('[','')\n",
    "df['main']= df['main'].str.replace(']','')\n",
    "df['main']= df['main'].str.replace('동아일보','')\n",
    "df['main']= df['main'].str.replace('동아닷컴','')\n",
    "df.dropna(how='any',inplace=True) \n",
    "#df['main']= df['main'].astype('str')\n",
    "data = {'time': df.time.unique(),'main' : df.groupby(['time'])['main'].apply(lambda x: \"%s\" % ', '.join(x)).values}\n",
    "df = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/konlpy/tag/_okt.py:16: UserWarning:\n",
      "\n",
      "\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "tagger = Twitter()\n",
    "\n",
    "def kor_noun(text):\n",
    "    words = []\n",
    "    for w in tagger.nouns(text):\n",
    "        if len(w) > 1:\n",
    "            words.append(w)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "cv = CountVectorizer(tokenizer=kor_noun, max_features=1000)\n",
    "articles = df['main']\n",
    "tdm = cv.fit_transform(articles)\n",
    "words = cv.get_feature_names()\n",
    "corpus = Sparse2Corpus(tdm.T)\n",
    "\n",
    "dictionary = Dictionary.from_corpus(\n",
    "    corpus,\n",
    "    id2word = dict(enumerate(words))\n",
    ")\n",
    "\n",
    "lda = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "               num_topics=1, passes=10, iterations=50,\n",
    "               random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pyLDAvis/_prepare.py:257: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3e724840da06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    398\u001b[0m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m    \u001b[0mclient_topic_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_order\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_coordinates\u001b[0;34m(mds, topic_term_dists, topic_proportion)\u001b[0m\n\u001b[1;32m    182\u001b[0m    \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m    \u001b[0mmds_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m    \u001b[0;32massert\u001b[0m \u001b[0mmds_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m    mds_df = pd.DataFrame({'x': mds_res[:,0], 'y': mds_res[:,1], 'topics': range(1, K + 1), \\\n\u001b[1;32m    186\u001b[0m                           'cluster': 1, 'Freq': topic_proportion * 100})\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "\n",
    "prepared_data = gensimvis.prepare(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기사 당 주제어 10개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_lists[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,pd.DataFrame(np.array(topic_lists),columns=['top1','top2','top3','top4','top5','top6','top7','top8','top9','top10'])],axis=1)\n",
    "df = pd.concat([df[['time']],pd.concat([pd.get_dummies(df.top1),pd.get_dummies(df.top2),pd.get_dummies(df.top3)\n",
    "          ,pd.get_dummies(df.top4),pd.get_dummies(df.top5),pd.get_dummies(df.top6)\n",
    "          ,pd.get_dummies(df.top7),pd.get_dummies(df.top8),pd.get_dummies(df.top9)\n",
    "          ,pd.get_dummies(df.top10)], axis=1, sort=False).fillna(0)],axis=1)\n",
    "df = df.sum(axis=1, level=0)\n",
    "df['time'] = pd.to_datetime(data['time'], format='%Y.%m.%d.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WTI = pd.read_csv('DCOILWTICO.csv', encoding='UTF-8')\n",
    "df_WTI.columns = ['time','price']\n",
    "df_WTI = df_WTI.drop(df.index[list(df_WTI[df_WTI.price=='.'].index)])\n",
    "df_WTI['price']= df_WTI.price.astype('float')\n",
    "df=df.merge(df_oil,how='outer',on=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 주유소 기름 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil = pd.read_csv('주유소_평균판매가격_제품별.csv',encoding='UTF-8')\n",
    "df_oil['time'] = pd.to_datetime(df_oil['구분'], format='%Y년%m월%d일')\n",
    "df = df_oil[['time','고급휘발유','보통휘발유','자동차용경유']].merge(df,on='time',how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다우존스 / 코스피 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dji = pd.read_csv('다우존스 내역.csv', encoding='UTF-8')\n",
    "kospi = pd.read_csv('코스피지수 내역.csv', encoding='UTF-8')\n",
    "df_dji['날짜'] = pd.to_datetime(df_dji['날짜'], format='%Y년 %m월 %d일')\n",
    "kospi['날짜'] = pd.to_datetime(kospi['날짜'], format='%Y년 %m월 %d일')\n",
    "df_dji['현재가'] = df_dji['현재가'].str.replace(',','').astype('float')\n",
    "kospi['현재가'] = kospi['현재가'].str.replace(',','').astype('float')\n",
    "df_stock = df_dji[['날짜','현재가']].merge(kospi[['날짜','현재가']],on='날짜',how='outer')\n",
    "df_stock['현재가_x'] = df_stock['현재가_x'].fillna(df_stock.현재가_x.astype('float32').mean())\n",
    "df_stock['현재가_y'] = df_stock['현재가_y'].fillna(df_stock.현재가_y.astype('float32').mean())\n",
    "df_stock['time'] = df_stock['날짜'].dt.strftime('%Y-%m-%d')\n",
    "df['time'] = df['time'].dt.strftime('%Y-%m-%d')\n",
    "df = df.merge(df_stock[['time','현재가_x','현재가_y']],how='outer',on=\"time\")\n",
    "df['현재가_x'] = df['현재가_x'].fillna(df.현재가_x.astype('float32').mean())\n",
    "df['현재가_y'] = df['현재가_y'].fillna(df.현재가_y.astype('float32').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환율 Data(USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd_list = []\n",
    "for date in list(df.time.unique()):\n",
    "    url = 'https://www.koreaexim.go.kr/site/program/financial/exchangeJSON?authkey=E24WPCtlRO2NyplVreVjosPe4XmFFtmb&searchdate={}&data=AP01'.format(date)\n",
    "    print('{} search start'.format(date))\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        print('retry')\n",
    "        response = requests.get(url)\n",
    "    if len(response.json()) == 0:\n",
    "        usd_list.append(None)\n",
    "        print('Null')\n",
    "    else:\n",
    "        for i in response.json():\n",
    "            if i['cur_unit'] == 'USD':\n",
    "                usd_list.append(i['ttb'])\n",
    "                print(i['ttb'])\n",
    "    print('{} search end'.format(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'er':usd_list,'time':df.time.unique()}).to_csv('er.csv',encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_er = pd.read_csv('er.csv',encoding='UTF-8',index_col=0)\n",
    "df_er['er'] = df_er['er'].str.replace(',','').astype('float')\n",
    "df_er['er'] = df_er['er'].fillna(df_er['er'].mean())\n",
    "df_er['time'] = pd.to_datetime(df_er['time'], format='%Y-%m-%d').astype('str')\n",
    "df = df.merge(df_er,on='time',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.5/dist-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.5/dist-packages (from bs4) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.5/dist-packages (from beautifulsoup4->bs4) (1.9.1)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "096770 start\n",
      "010950 start\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "import urllib\n",
    "import time\n",
    " \n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "total_lists =[]\n",
    "\n",
    "stockItems = ['096770','010950','011780','020560','003490','004870']\n",
    "\n",
    "for stockItem in stockItems:\n",
    "    timestamp = []\n",
    "    data = []\n",
    "    print('{} start'.format(stockItem))\n",
    "    url = 'http://finance.naver.com/item/sise_day.nhn?code='+ stockItem\n",
    "    html = urlopen(url) \n",
    "    source = BeautifulSoup(html.read(), \"html.parser\")\n",
    "    maxPage=source.find_all(\"table\",align=\"center\")\n",
    "    mp = maxPage[0].find_all(\"td\",class_=\"pgRR\")\n",
    "    #mpNum = int(mp[0].a.get('href')[-3:])\n",
    "    mpNum = 283\n",
    "                                         \n",
    "    for page in range(1, mpNum+1):\n",
    "        url = 'http://finance.naver.com/item/sise_day.nhn?code=' + stockItem +'&page='+ str(page)\n",
    "        html = urlopen(url)\n",
    "        source = BeautifulSoup(html.read(), \"html.parser\")\n",
    "        srlists=source.find_all(\"tr\")\n",
    "        isCheckNone = None\n",
    "        if((page % 1) == 0):\n",
    "            time.sleep(1.50)\n",
    "    for i in range(1,len(srlists)-1):\n",
    "        if(srlists[i].span != isCheckNone):\n",
    "            srlists[i].td.text\n",
    "            timestamp.append(srlists[i].find_all(\"td\",align=\"center\")[0].text)\n",
    "            data.append(srlists[i].find_all(\"td\",class_=\"num\")[0].text )\n",
    "    df = pd.DataFrame(data={'time':timestamp,'S-OIL':data})\n",
    "    total_lists.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[labels] = df[labels].fillna(0)\n",
    "df[labels].isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.drop(labels,axis=1).columns:\n",
    "    df[i] = df[i].fillna(df[i].mean())\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any').set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerX = MinMaxScaler()\n",
    "x = scalerX.fit_transform(df)\n",
    "scalery = MinMaxScaler()\n",
    "y = scalery.fit_transform(df.iloc[:,[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y)\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer comes here.\n",
    "SEQ_LEN = 5\n",
    "\n",
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(y) - SEQ_LEN):\n",
    "    _x = x[i:i + SEQ_LEN]\n",
    "    _y = y[i + SEQ_LEN]\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "dataX = np.array(dataX)\n",
    "dataY = np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataY) * 0.8)\n",
    "test_size = len(dataY) - train_size\n",
    "Xtrain, Xtest = np.array(dataX[0:train_size]), np.array(dataX[train_size:len(dataX)])\n",
    "ytrain, ytest = np.array(dataY[0:train_size]), np.array(dataY[train_size:len(dataY)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Xtrain), np.shape(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer comes here.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(5, 954),return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss = 'mse', optimizer = 'adam', metrics = ['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer comes here.\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "now = time.strftime(\"%c\")\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(filepath='stock'+now+'.h5',monitor='val_loss',save_best_only=True)\n",
    "]\n",
    "history=model.fit(Xtrain, ytrain, validation_data=(Xtest,ytest),epochs=20, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('stockSat Jun  8 07:24:44 2019.h5')  # best model 불러오기\n",
    "test_predict = model.predict(Xtest)\n",
    "test_predict_back = scalery.inverse_transform(test_predict)\n",
    "ytest_back = scalery.inverse_transform(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.plot(ytest_back, label = 'Y')\n",
    "plt.plot(test_predict_back, label = 'Y_predicted')\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(test_predict_back, ytest_back), np.mean(test_predict_back / ytest_back) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(Xtrain)\n",
    "test_predict_back = scalery.inverse_transform(test_predict)\n",
    "ytest_back = scalery.inverse_transform(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.plot(ytest_back, label = 'Y')\n",
    "plt.plot(test_predict_back, label = 'Y_predicted')\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
